{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2238447b",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbf6af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded merged data: 5812149 rows, 58 columns\n",
      "Loaded FEMA data: 20000 rows, 9 columns\n",
      "\n",
      "Data Quality Report:\n",
      "Merged data: 5812149 rows, 58 columns\n",
      "\n",
      "Found 51 available states\n",
      "Sample states: ALABAMA, ALASKA, ARIZONA, ARKANSAS, CALIFORNIA, COLORADO, COLUMBIA, CONNECTICUT, DELAWARE, FLORIDA\n",
      "\n",
      "State CONNECTICUT has 215 cities\n",
      "Cities with only 1 record: 0\n",
      "Cities with fewer than 5 records: 1\n",
      "\n",
      "Top cities by record count:\n",
      "- Middletown: 671 records\n",
      "- Hartford: 637 records\n",
      "- Bristol: 636 records\n",
      "- Norwich: 636 records\n",
      "- Manchester: 635 records\n",
      "\n",
      "Sample cities: Ansonia, Ball Pond, Baltic, Bantam, Bethel\n",
      "\n",
      "Testing with Bantam, CONNECTICUT\n",
      "Found 205 records\n",
      "Average price: $297616.78\n",
      "Price volatility: 1.1748\n",
      "Years covered: 14\n",
      "\n",
      "Disaster summary for CONNECTICUT:\n",
      "Total disasters: 69155\n",
      "Yearly average: 4939.64\n",
      "Years covered: 14\n",
      "Risk level: high\n",
      "Source: Merged dataset (count of all records)\n",
      "\n",
      "Disaster summary for Bantam, CONNECTICUT:\n",
      "Total disasters: 205\n",
      "Yearly average: 14.64\n",
      "Years covered: 14\n",
      "Risk level: medium\n",
      "Source: Merged dataset (city-specific record count)\n",
      "\n",
      "This city's contribution to state total: 205 disasters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Class for loading and processing real estate and disaster data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the data processor\"\"\"\n",
    "        self.redfin_df = None\n",
    "        self.fema_df = None\n",
    "        self.merged_df = None\n",
    "    \n",
    "    def load_merged_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load the pre-merged Redfin and FEMA data from CSV file\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the merged data file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with processed merged data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the data\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            # Convert date columns to datetime (without timezone)\n",
    "            date_cols = ['PERIOD_BEGIN', 'PERIOD_END']\n",
    "            for col in date_cols:\n",
    "                if col in df.columns:\n",
    "                    # First check if timezone info exists and remove it\n",
    "                    temp_dates = pd.to_datetime(df[col], errors='coerce')\n",
    "                    if temp_dates.dt.tz is not None:\n",
    "                        # Convert to naive datetime (no timezone)\n",
    "                        df[col] = temp_dates.dt.tz_localize(None)\n",
    "                    else:\n",
    "                        df[col] = temp_dates\n",
    "            \n",
    "            # Add year column if not present\n",
    "            if 'YEAR' not in df.columns and 'PERIOD_BEGIN' in df.columns:\n",
    "                df['YEAR'] = df['PERIOD_BEGIN'].dt.year\n",
    "            \n",
    "            # Standardize state and city names\n",
    "            if 'STATE' in df.columns:\n",
    "                df['STATE'] = df['STATE'].astype(str).str.upper().str.strip()\n",
    "            \n",
    "            if 'CITY' in df.columns:\n",
    "                df['CITY'] = df['CITY'].astype(str).str.title().str.strip()\n",
    "            \n",
    "            # Fill missing values for important columns\n",
    "            numeric_cols = [\n",
    "                'MEDIAN_SALE_PRICE', 'INVENTORY', 'HOMES_SOLD', \n",
    "                'NEW_LISTINGS', 'MEDIAN_DOM', 'AVG_SALE_TO_LIST', 'natural_disaster_score'\n",
    "            ]\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    # For each column, fill NaNs with median for that city/state group\n",
    "                    df[col] = df.groupby(['STATE', 'CITY'])[col].transform(\n",
    "                        lambda x: x.fillna(x.median() if not pd.isna(x.median()) else 0)\n",
    "                    )\n",
    "                    \n",
    "                    # For any remaining NaNs, fill with 0\n",
    "                    df[col] = df[col].fillna(0)\n",
    "            \n",
    "            # Note any data quality issues but don't modify the data\n",
    "            if 'natural_disaster_score' in df.columns:\n",
    "                max_disasters = df['natural_disaster_score'].max()\n",
    "                if max_disasters > 1000:\n",
    "                    print(f\"WARNING: Extremely high disaster counts detected (max: {max_disasters})\")\n",
    "                    print(\"These values are being preserved in the data but may indicate data quality issues.\")\n",
    "            \n",
    "            self.merged_df = df\n",
    "            print(f\"Loaded merged data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading merged data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_fema_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load FEMA disaster data from CSV file for additional analysis\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the FEMA data file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with processed FEMA data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the data\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            # Convert date columns to datetime (without timezone)\n",
    "            date_cols = ['declarationdate', 'incidentbegindate', 'incidentenddate']\n",
    "            for col in date_cols:\n",
    "                if col in df.columns:\n",
    "                    # First check if timezone info exists and remove it\n",
    "                    temp_dates = pd.to_datetime(df[col], errors='coerce')\n",
    "                    if temp_dates.dt.tz is not None:\n",
    "                        # Convert to naive datetime (no timezone)\n",
    "                        df[col] = temp_dates.dt.tz_localize(None)\n",
    "                    else:\n",
    "                        df[col] = temp_dates\n",
    "            \n",
    "            # Standardize text columns\n",
    "            if 'state' in df.columns:\n",
    "                df['state'] = df['state'].astype(str).str.upper().str.strip()\n",
    "                \n",
    "            if 'incidenttype' in df.columns:\n",
    "                df['incidenttype'] = df['incidenttype'].astype(str).str.title().str.strip()\n",
    "            \n",
    "            # Add year column if not present\n",
    "            if 'incidentbegindate' in df.columns:\n",
    "                df['year'] = df['incidentbegindate'].dt.year\n",
    "            \n",
    "            self.fema_df = df\n",
    "            print(f\"Loaded FEMA data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading FEMA data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_location_data(self, state, city):\n",
    "        \"\"\"\n",
    "        Get data for a specific location from the merged dataset\n",
    "        \n",
    "        Args:\n",
    "            state: State code\n",
    "            city: City name\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with location-specific data\n",
    "        \"\"\"\n",
    "        if self.merged_df is not None:\n",
    "            # Filter for location\n",
    "            location_data = self.merged_df[\n",
    "                (self.merged_df['STATE'] == state.upper()) &\n",
    "                (self.merged_df['CITY'] == city.title())\n",
    "            ]\n",
    "            \n",
    "            if location_data.empty:\n",
    "                print(f\"No data found for {city}, {state}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            return location_data\n",
    "        else:\n",
    "            print(\"Error: No merged data loaded\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_state_disaster_data(self, state):\n",
    "        \"\"\"\n",
    "        Get disaster data for a specific state from the FEMA dataset\n",
    "        \n",
    "        Args:\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with state-specific disaster data\n",
    "        \"\"\"\n",
    "        if self.fema_df is None:\n",
    "            print(\"Error: FEMA data not loaded\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            # Filter for state (no year filtering)\n",
    "            state_disasters = self.fema_df[self.fema_df['state'] == state.upper()]\n",
    "            return state_disasters\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_state_disaster_data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_state_disaster_summary(self, state):\n",
    "        \"\"\"\n",
    "        Get summary of disasters for the entire state using all available years\n",
    "\n",
    "        Args:\n",
    "            state: State code\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with disaster metrics\n",
    "        \"\"\"\n",
    "        # First, try to use the FEMA dataset directly (more accurate)\n",
    "        if self.fema_df is not None:\n",
    "            fema_state_data = self.get_state_disaster_data(state)\n",
    "\n",
    "            if not fema_state_data.empty:\n",
    "                # Count disasters from FEMA data\n",
    "                total_disasters = len(fema_state_data)\n",
    "\n",
    "                # Determine years covered\n",
    "                if 'year' in fema_state_data.columns:\n",
    "                    years = fema_state_data['year'].unique()\n",
    "                    years_covered = max(years) - min(years) + 1\n",
    "                else:\n",
    "                    years_covered = 14  # Based on your data from 2012-2025\n",
    "\n",
    "                # Calculate yearly average\n",
    "                yearly_avg = total_disasters / years_covered\n",
    "\n",
    "                # Get disaster types\n",
    "                disaster_types = {}\n",
    "                if 'incidenttype' in fema_state_data.columns:\n",
    "                    disaster_types = fema_state_data['incidenttype'].value_counts().to_dict()\n",
    "\n",
    "                # Determine risk level\n",
    "                if yearly_avg > 20:\n",
    "                    risk_level = \"high\"\n",
    "                elif yearly_avg > 10:\n",
    "                    risk_level = \"medium\"\n",
    "                else:\n",
    "                    risk_level = \"low\"\n",
    "\n",
    "                return {\n",
    "                    \"total_disasters\": total_disasters,\n",
    "                    \"yearly_avg\": yearly_avg,\n",
    "                    \"years_covered\": years_covered,\n",
    "                    \"disaster_types\": disaster_types,\n",
    "                    \"risk_level\": risk_level,\n",
    "                    \"source\": \"FEMA dataset\"\n",
    "                }\n",
    "\n",
    "        # If FEMA data isn't available or was empty, fall back to merged dataset\n",
    "        if self.merged_df is not None:\n",
    "            try:\n",
    "                # Get state data (without year filtering)\n",
    "                state_data = self.merged_df[self.merged_df['STATE'] == state.upper()]\n",
    "\n",
    "                if not state_data.empty:\n",
    "                    # Calculate years covered from the data or use provided range\n",
    "                    if 'YEAR' in state_data.columns:\n",
    "                        years = state_data['YEAR'].unique()\n",
    "                        years_covered = max(years) - min(years) + 1\n",
    "                    else:\n",
    "                        years_covered = 14  # Based on your data from 2012-2025\n",
    "\n",
    "                    # Count total records for state - this should match your Excel count\n",
    "                    total_disasters = len(state_data)\n",
    "\n",
    "                    # Calculate city-specific record counts\n",
    "                    city_disasters = state_data.groupby('CITY').size().to_dict()\n",
    "\n",
    "                    # Calculate yearly average for the state\n",
    "                    yearly_avg = total_disasters / years_covered\n",
    "\n",
    "                    # Determine risk level for the state\n",
    "                    if yearly_avg > 20:\n",
    "                        risk_level = \"high\"\n",
    "                    elif yearly_avg > 10:\n",
    "                        risk_level = \"medium\"\n",
    "                    else:\n",
    "                        risk_level = \"low\"\n",
    "\n",
    "                    return {\n",
    "                        \"total_disasters\": total_disasters,\n",
    "                        \"yearly_avg\": yearly_avg,\n",
    "                        \"years_covered\": years_covered,\n",
    "                        \"disaster_types\": {},  # No types available from merged data\n",
    "                        \"risk_level\": risk_level,\n",
    "                        \"source\": \"Merged dataset (count of all records)\",\n",
    "                        \"city_disasters\": city_disasters  # City-specific record counts\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating from merged data: {e}\")\n",
    "\n",
    "        # If all else fails, return default values\n",
    "        return {\n",
    "            \"total_disasters\": 0,\n",
    "            \"yearly_avg\": 0,\n",
    "            \"years_covered\": 0,\n",
    "            \"disaster_types\": {},\n",
    "            \"risk_level\": \"unknown\",\n",
    "            \"source\": \"No data available\"\n",
    "        }\n",
    "    \n",
    "    def get_city_disaster_summary(self, city, state):\n",
    "        \"\"\"\n",
    "        Get summary of disasters for a specific city using all available years\n",
    "\n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with disaster metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get city data (without year filtering)\n",
    "            city_data = self.merged_df[\n",
    "                (self.merged_df['STATE'] == state.upper()) &\n",
    "                (self.merged_df['CITY'] == city.title())\n",
    "            ]\n",
    "\n",
    "            if not city_data.empty:\n",
    "                # Calculate years covered from the data or use provided range\n",
    "                if 'YEAR' in city_data.columns:\n",
    "                    years = city_data['YEAR'].unique()\n",
    "                    years_covered = max(years) - min(years) + 1\n",
    "                else:\n",
    "                    years_covered = 14  # Based on your data from 2012-2025\n",
    "\n",
    "                # Count records as total disasters\n",
    "                total_disasters = len(city_data)\n",
    "\n",
    "                # Calculate yearly average for this city\n",
    "                yearly_avg = total_disasters / years_covered if years_covered > 0 else 0\n",
    "\n",
    "                # Determine risk level for this city\n",
    "                if yearly_avg > 20:\n",
    "                    risk_level = \"high\"\n",
    "                elif yearly_avg > 10:\n",
    "                    risk_level = \"medium\"\n",
    "                else:\n",
    "                    risk_level = \"low\"\n",
    "\n",
    "                return {\n",
    "                    \"total_disasters\": total_disasters,\n",
    "                    \"yearly_avg\": yearly_avg,\n",
    "                    \"years_covered\": years_covered,\n",
    "                    \"risk_level\": risk_level,\n",
    "                    \"source\": \"Merged dataset (city-specific record count)\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating city disaster summary: {e}\")\n",
    "\n",
    "        # If any errors or no data, return default values\n",
    "        return {\n",
    "            \"total_disasters\": 0,\n",
    "            \"yearly_avg\": 0,\n",
    "            \"years_covered\": 0,\n",
    "            \"risk_level\": \"unknown\",\n",
    "            \"source\": \"No data available for this city\"\n",
    "        }\n",
    "    \n",
    "    def get_available_states(self):\n",
    "        \"\"\"Get list of available states in the data\"\"\"\n",
    "        if self.merged_df is not None and 'STATE' in self.merged_df.columns:\n",
    "            # Group by STATE to find states with data\n",
    "            state_counts = self.merged_df.groupby('STATE').size()\n",
    "            states = state_counts.index.tolist()\n",
    "            return sorted(states)\n",
    "        return []\n",
    "    \n",
    "    def get_available_cities(self, state):\n",
    "        \"\"\"\n",
    "        Get list of all available cities for a state\n",
    "        \n",
    "        Args:\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            List of city names\n",
    "        \"\"\"\n",
    "        if self.merged_df is not None and 'CITY' in self.merged_df.columns and 'STATE' in self.merged_df.columns:\n",
    "            # Group by CITY within this state to find cities with data\n",
    "            state_data = self.merged_df[self.merged_df['STATE'] == state.upper()]\n",
    "            city_counts = state_data.groupby('CITY').size()\n",
    "            cities = city_counts.index.tolist()\n",
    "            city_count = len(cities)\n",
    "            \n",
    "            # Print information about city counts for transparency\n",
    "            if city_count > 500:\n",
    "                print(f\"Note: {state} has {city_count} unique cities in dataset.\")\n",
    "                print(\"This is more than the expected number of municipalities and may include\")\n",
    "                print(\"neighborhoods, misspellings, or other data quality issues.\")\n",
    "                \n",
    "            return sorted(cities)\n",
    "        return []\n",
    "    \n",
    "    def get_city_stats(self, state):\n",
    "        \"\"\"\n",
    "        Get detailed statistics about cities in a state\n",
    "\n",
    "        Args:\n",
    "            state: State code\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with city statistics\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            \"total_cities\": 0,\n",
    "            \"cities_with_1_record\": 0,\n",
    "            \"cities_with_few_records\": 0,\n",
    "            \"top_cities_by_records\": []\n",
    "        }\n",
    "\n",
    "        if self.merged_df is not None and 'CITY' in self.merged_df.columns and 'STATE' in self.merged_df.columns:\n",
    "            # Filter data to only include records from this state\n",
    "            state_data = self.merged_df[self.merged_df['STATE'] == state.upper()]\n",
    "\n",
    "            # Get unique cities in this state\n",
    "            unique_cities = state_data['CITY'].unique()\n",
    "\n",
    "            # Count records per city\n",
    "            city_counts = state_data.groupby('CITY').size()\n",
    "\n",
    "            stats[\"total_cities\"] = len(unique_cities)  # Number of unique cities\n",
    "            stats[\"cities_with_1_record\"] = sum(city_counts == 1)\n",
    "            stats[\"cities_with_few_records\"] = sum(city_counts < 5)\n",
    "\n",
    "            # Get top cities by record count\n",
    "            top_cities = city_counts.sort_values(ascending=False).head(10)\n",
    "            stats[\"top_cities_by_records\"] = [{\"city\": city, \"records\": count} for city, count in top_cities.items()]\n",
    "\n",
    "        return stats\n",
    "    \n",
    "    def calculate_city_price_metrics(self, city, state):\n",
    "        \"\"\"\n",
    "        Calculate price metrics for a specific city\n",
    "        \n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with price metrics\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            \"avg_price\": 0,\n",
    "            \"median_price\": 0,\n",
    "            \"min_price\": 0,\n",
    "            \"max_price\": 0,\n",
    "            \"price_volatility\": 0,\n",
    "            \"record_count\": 0,\n",
    "            \"years_covered\": 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Get location data for this specific city\n",
    "            location_data = self.get_location_data(state, city)\n",
    "            \n",
    "            if location_data.empty or 'MEDIAN_SALE_PRICE' not in location_data.columns:\n",
    "                return metrics\n",
    "            \n",
    "            # Calculate years covered from the data\n",
    "            if 'YEAR' in location_data.columns:\n",
    "                years = location_data['YEAR'].unique()\n",
    "                metrics[\"years_covered\"] = max(years) - min(years) + 1\n",
    "            \n",
    "            # Calculate price metrics for this specific city\n",
    "            prices = location_data['MEDIAN_SALE_PRICE'].dropna()\n",
    "            \n",
    "            metrics[\"avg_price\"] = prices.mean()\n",
    "            metrics[\"median_price\"] = prices.median()\n",
    "            metrics[\"min_price\"] = prices.min()\n",
    "            metrics[\"max_price\"] = prices.max()\n",
    "            metrics[\"record_count\"] = len(location_data)\n",
    "            \n",
    "            # Calculate price volatility\n",
    "            location_data = location_data.sort_values('PERIOD_BEGIN')\n",
    "            price_changes = location_data['MEDIAN_SALE_PRICE'].pct_change().dropna()\n",
    "            \n",
    "            if len(price_changes) >= 2:\n",
    "                metrics[\"price_volatility\"] = price_changes.std()\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating city price metrics: {e}\")\n",
    "            return metrics\n",
    "    \n",
    "    def calculate_price_volatility(self, city, state):\n",
    "        \"\"\"\n",
    "        Calculate price volatility for a specific city\n",
    "        \n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            Volatility score (standard deviation of price changes)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # This should only use data for the specific city\n",
    "            location_data = self.get_location_data(state, city)\n",
    "            \n",
    "            if location_data.empty or 'MEDIAN_SALE_PRICE' not in location_data.columns:\n",
    "                return 0.05  # Default value\n",
    "            \n",
    "            # Sort by date\n",
    "            location_data = location_data.sort_values('PERIOD_BEGIN')\n",
    "            \n",
    "            # Calculate price changes\n",
    "            price_changes = location_data['MEDIAN_SALE_PRICE'].pct_change().dropna()\n",
    "            \n",
    "            if len(price_changes) < 2:\n",
    "                return 0.05  # Default value\n",
    "            \n",
    "            # Return standard deviation as volatility measure\n",
    "            return price_changes.std()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating price volatility: {e}\")\n",
    "            return 0.05  # Default value\n",
    "    \n",
    "    def get_data_quality_report(self):\n",
    "        \"\"\"\n",
    "        Generate a data quality report for the loaded data\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with data quality metrics\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            \"merged_data\": {\n",
    "                \"rows\": 0,\n",
    "                \"columns\": 0,\n",
    "                \"missing_values\": {},\n",
    "                \"suspicious_values\": {}\n",
    "            },\n",
    "            \"fema_data\": {\n",
    "                \"rows\": 0,\n",
    "                \"columns\": 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Check merged data\n",
    "        if self.merged_df is not None:\n",
    "            report[\"merged_data\"][\"rows\"] = len(self.merged_df)\n",
    "            report[\"merged_data\"][\"columns\"] = len(self.merged_df.columns)\n",
    "            \n",
    "            # Check for missing values\n",
    "            missing = self.merged_df.isnull().sum()\n",
    "            missing = missing[missing > 0]\n",
    "            report[\"merged_data\"][\"missing_values\"] = missing.to_dict()\n",
    "            \n",
    "            # Check for suspicious disaster counts\n",
    "            if 'natural_disaster_score' in self.merged_df.columns:\n",
    "                suspicious = self.merged_df[self.merged_df['natural_disaster_score'] > 100]\n",
    "                if not suspicious.empty:\n",
    "                    report[\"merged_data\"][\"suspicious_values\"][\"high_disaster_count\"] = len(suspicious)\n",
    "        \n",
    "        # Check FEMA data\n",
    "        if self.fema_df is not None:\n",
    "            report[\"fema_data\"][\"rows\"] = len(self.fema_df)\n",
    "            report[\"fema_data\"][\"columns\"] = len(self.fema_df.columns)\n",
    "            \n",
    "        return report\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create data processor instance\n",
    "    processor = DataProcessor()\n",
    "    \n",
    "    # Load the merged dataset that contains both Redfin and FEMA data\n",
    "    merged_data = processor.load_merged_data(\"/Users/shivangi/Downloads/redfin_with_disaster_score.csv\")\n",
    "    \n",
    "    # Optionally load separate FEMA data for more detailed disaster analysis\n",
    "    fema_data = processor.load_fema_data(\"/Users/shivangi/Downloads/fema_cleaned.csv\")\n",
    "    \n",
    "    # Get data quality report\n",
    "    quality_report = processor.get_data_quality_report()\n",
    "    print(\"\\nData Quality Report:\")\n",
    "    print(f\"Merged data: {quality_report['merged_data']['rows']} rows, {quality_report['merged_data']['columns']} columns\")\n",
    "    if quality_report['merged_data']['suspicious_values']:\n",
    "        print(\"Found suspicious values:\")\n",
    "        for key, value in quality_report['merged_data']['suspicious_values'].items():\n",
    "            print(f\"- {key}: {value} records\")\n",
    "    \n",
    "    # Get available states and print the count\n",
    "    available_states = processor.get_available_states()\n",
    "    print(f\"\\nFound {len(available_states)} available states\")\n",
    "    \n",
    "    if available_states:\n",
    "        # Print first few states\n",
    "        print(f\"Sample states: {', '.join(available_states[:10])}\")\n",
    "        \n",
    "        # Test with the first available state\n",
    "        test_state = available_states[7]\n",
    "        available_cities = processor.get_available_cities(test_state)\n",
    "        \n",
    "        # Get city stats\n",
    "        city_stats = processor.get_city_stats(test_state)\n",
    "        print(f\"\\nState {test_state} has {city_stats['total_cities']} cities\")\n",
    "        print(f\"Cities with only 1 record: {city_stats['cities_with_1_record']}\")\n",
    "        print(f\"Cities with fewer than 5 records: {city_stats['cities_with_few_records']}\")\n",
    "        \n",
    "        print(f\"\\nTop cities by record count:\")\n",
    "        for city_data in city_stats['top_cities_by_records'][:5]:\n",
    "            print(f\"- {city_data['city']}: {city_data['records']} records\")\n",
    "        \n",
    "        if available_cities:\n",
    "            # Print first few cities\n",
    "            print(f\"\\nSample cities: {', '.join(available_cities[:5])}\")\n",
    "            \n",
    "            # Test with the first available city\n",
    "            test_city = available_cities[3]\n",
    "            print(f\"\\nTesting with {test_city}, {test_state}\")\n",
    "            \n",
    "            # Get location data\n",
    "            location_data = processor.get_location_data(test_state, test_city)\n",
    "            if not location_data.empty:\n",
    "                # Print basic stats\n",
    "                print(f\"Found {len(location_data)} records\")\n",
    "                \n",
    "                # Calculate city-specific price metrics\n",
    "                price_metrics = processor.calculate_city_price_metrics(test_city, test_state)\n",
    "                print(f\"Average price: ${price_metrics['avg_price']:.2f}\")\n",
    "                print(f\"Price volatility: {price_metrics['price_volatility']:.4f}\")\n",
    "                print(f\"Years covered: {price_metrics['years_covered']}\")\n",
    "            \n",
    "            # Get disaster summary for the state\n",
    "            disaster_summary = processor.get_state_disaster_summary(test_state)\n",
    "            print(f\"\\nDisaster summary for {test_state}:\")\n",
    "            print(f\"Total disasters: {disaster_summary['total_disasters']}\")\n",
    "            print(f\"Yearly average: {disaster_summary['yearly_avg']:.2f}\")\n",
    "            print(f\"Years covered: {disaster_summary['years_covered']}\")\n",
    "            print(f\"Risk level: {disaster_summary['risk_level']}\")\n",
    "            print(f\"Source: {disaster_summary['source']}\")\n",
    "            \n",
    "            # Print disaster types if available\n",
    "            if disaster_summary['disaster_types']:\n",
    "                print(\"\\nTop disaster types:\")\n",
    "                top_disasters = sorted(disaster_summary['disaster_types'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                for disaster_type, count in top_disasters:\n",
    "                    print(f\"- {disaster_type}: {count}\")\n",
    "            \n",
    "            if 'note' in disaster_summary:\n",
    "                print(f\"\\nNote: {disaster_summary['note']}\")\n",
    "            \n",
    "            # Get city-specific disaster summary (new functionality)\n",
    "            city_disaster_summary = processor.get_city_disaster_summary(test_city, test_state)\n",
    "            print(f\"\\nDisaster summary for {test_city}, {test_state}:\")\n",
    "            print(f\"Total disasters: {city_disaster_summary['total_disasters']}\")\n",
    "            print(f\"Yearly average: {city_disaster_summary['yearly_avg']:.2f}\")\n",
    "            print(f\"Years covered: {city_disaster_summary['years_covered']}\")\n",
    "            print(f\"Risk level: {city_disaster_summary['risk_level']}\")\n",
    "            print(f\"Source: {city_disaster_summary['source']}\")\n",
    "            \n",
    "            # If the state summary has city-specific disaster counts, show this city's contribution\n",
    "            if 'city_disasters' in disaster_summary and test_city.title() in disaster_summary['city_disasters']:\n",
    "                city_count = disaster_summary['city_disasters'][test_city.title()]\n",
    "                print(f\"\\nThis city's contribution to state total: {city_count} disasters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a60ab069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing weather data for Austin, TX...\n",
      "Fetching hourly weather data for Austin, TX...\n",
      "No data returned from API. Using generated sample data.\n",
      "Generating sample hourly weather data for Austin, TX...\n",
      "Generated 8761 hours of sample weather data\n",
      "\n",
      "Weather Risk Assessment: LOW\n",
      "Risk Score: 28.0/100\n",
      "\n",
      "Risk Factors:\n",
      "- High heat: 80.8 days above 90°F per year\n",
      "- High precipitation: 50.3 inches annually\n",
      "- Poor comfort: Only 19.7% of hours have ideal conditions\n",
      "\n",
      "Positive Factors:\n",
      "- Minimal freezing days\n",
      "- Low humidity levels\n",
      "Using cached hourly weather data for Austin, TX\n",
      "Error in testing: Can only use .dt accessor with datetimelike values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/nsb_559s3znd30djr8w76s1w0000gn/T/ipykernel_12355/2598282420.py:384: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  season_temps = df.groupby('season')['temperature'].mean().to_dict()\n",
      "/var/folders/2n/nsb_559s3znd30djr8w76s1w0000gn/T/ipykernel_12355/2598282420.py:388: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  season_precip = df.groupby('season')['precipitation'].sum().to_dict()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class HourlyWeatherAnalyzer:\n",
    "    \"\"\"\n",
    "    Class for retrieving and analyzing hourly weather data from NOAA\n",
    "    to assess climate-related risks for real estate investments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, noaa_token=None):\n",
    "        \"\"\"\n",
    "        Initialize the weather analyzer\n",
    "        \n",
    "        Args:\n",
    "            noaa_token: API token for NOAA data access\n",
    "        \"\"\"\n",
    "        self.noaa_token = noaa_token\n",
    "        self.cache_dir = \"weather_cache\"\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        if not os.path.exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "    \n",
    "    def set_api_token(self, token):\n",
    "        \"\"\"\n",
    "        Set the NOAA API token\n",
    "        \n",
    "        Args:\n",
    "            token: API token\n",
    "        \"\"\"\n",
    "        self.noaa_token = token\n",
    "    \n",
    "    def get_hourly_weather_data(self, city, state):\n",
    "        \"\"\"\n",
    "        Get hourly weather data for a city for the past year\n",
    "        \n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with hourly weather data or dictionary with aggregated metrics if API fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check cache first\n",
    "            cache_file = os.path.join(self.cache_dir, f\"{state.upper()}_{city.title()}_hourly.csv\")\n",
    "            if os.path.exists(cache_file):\n",
    "                # Check if cache is fresh (less than 1 day old)\n",
    "                cache_time = datetime.fromtimestamp(os.path.getmtime(cache_file))\n",
    "                if (datetime.now() - cache_time).days < 1:\n",
    "                    print(f\"Using cached hourly weather data for {city}, {state}\")\n",
    "                    return pd.read_csv(cache_file)\n",
    "            \n",
    "            # Set date range for the past year\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=365)\n",
    "            \n",
    "            # Format dates for API\n",
    "            start_str = start_date.strftime('%Y-%m-%dT00:00:00')\n",
    "            end_str = end_date.strftime('%Y-%m-%dT23:59:59')\n",
    "            \n",
    "            if not self.noaa_token:\n",
    "                print(\"No NOAA API token provided. Using generated sample data.\")\n",
    "                # Generate sample hourly data\n",
    "                return self._generate_sample_hourly_data(city, state, start_date, end_date)\n",
    "            \n",
    "            print(f\"Fetching hourly weather data for {city}, {state}...\")\n",
    "            \n",
    "            # Prepare API request for hourly data\n",
    "            headers = {\"token\": self.noaa_token}\n",
    "            base_url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "            \n",
    "            # Use GHCND (Global Historical Climatology Network - Daily) dataset\n",
    "            # with hourly precipitation, temperature, and other elements\n",
    "            params = {\n",
    "                \"datasetid\": \"NORMAL_HLY\",  # Hourly Normals dataset\n",
    "                \"locationid\": f\"CITY:{city.upper()}\",\n",
    "                \"startdate\": start_str,\n",
    "                \"enddate\": end_str,\n",
    "                \"units\": \"standard\",\n",
    "                \"limit\": 1000\n",
    "            }\n",
    "            \n",
    "            # Make API request\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error fetching hourly data: {response.status_code}\")\n",
    "                print(\"Falling back to GHCND dataset with daily data...\")\n",
    "                \n",
    "                # Fall back to daily data (GHCND dataset)\n",
    "                params = {\n",
    "                    \"datasetid\": \"GHCND\",\n",
    "                    \"locationid\": f\"CITY:{city.upper()}\",\n",
    "                    \"startdate\": start_date.strftime('%Y-%m-%d'),\n",
    "                    \"enddate\": end_date.strftime('%Y-%m-%d'),\n",
    "                    \"units\": \"standard\",\n",
    "                    \"limit\": 1000\n",
    "                }\n",
    "                \n",
    "                response = requests.get(base_url, headers=headers, params=params)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Error fetching daily data: {response.status_code}\")\n",
    "                    print(\"Using generated sample data instead.\")\n",
    "                    # Generate sample data as fallback\n",
    "                    return self._generate_sample_hourly_data(city, state, start_date, end_date)\n",
    "            \n",
    "            # Parse response\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'results' not in data or not data['results']:\n",
    "                print(\"No data returned from API. Using generated sample data.\")\n",
    "                return self._generate_sample_hourly_data(city, state, start_date, end_date)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(data['results'])\n",
    "            \n",
    "            # Save to cache\n",
    "            df.to_csv(cache_file, index=False)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving weather data: {e}\")\n",
    "            print(\"Using generated sample data instead.\")\n",
    "            # Generate sample data as fallback\n",
    "            return self._generate_sample_hourly_data(city, state, start_date, end_date)\n",
    "    \n",
    "    def _generate_sample_hourly_data(self, city, state, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Generate sample hourly weather data for testing\n",
    "        \n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "            start_date: Start date\n",
    "            end_date: End date\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with sample hourly data\n",
    "        \"\"\"\n",
    "        print(f\"Generating sample hourly weather data for {city}, {state}...\")\n",
    "        \n",
    "        # Generate hourly timestamps for the entire year\n",
    "        hours = int((end_date - start_date).total_seconds() / 3600) + 1\n",
    "        timestamps = [start_date + timedelta(hours=i) for i in range(hours)]\n",
    "        \n",
    "        # Base parameters that vary by location (rough estimates)\n",
    "        location_params = {\n",
    "            'TX': {'temp_mean': 75, 'temp_amplitude': 20, 'precip_prob': 0.05, 'humid_mean': 60},\n",
    "            'FL': {'temp_mean': 80, 'temp_amplitude': 15, 'precip_prob': 0.15, 'humid_mean': 75},\n",
    "            'NY': {'temp_mean': 55, 'temp_amplitude': 25, 'precip_prob': 0.10, 'humid_mean': 65},\n",
    "            'CA': {'temp_mean': 65, 'temp_amplitude': 15, 'precip_prob': 0.05, 'humid_mean': 50},\n",
    "            'WA': {'temp_mean': 50, 'temp_amplitude': 15, 'precip_prob': 0.20, 'humid_mean': 70}\n",
    "        }\n",
    "        \n",
    "        # Use the state params or default if not found\n",
    "        params = location_params.get(state.upper(), \n",
    "                                     {'temp_mean': 65, 'temp_amplitude': 20, 'precip_prob': 0.10, 'humid_mean': 60})\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'date': timestamps,\n",
    "            'city': city,\n",
    "            'state': state\n",
    "        })\n",
    "        \n",
    "        # Add hour and month for seasonal patterns\n",
    "        df['hour'] = df['date'].dt.hour\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day'] = df['date'].dt.day\n",
    "        \n",
    "        # Generate temperature (°F) with daily and seasonal variations\n",
    "        # Daily cycle: lowest at ~5 AM, highest at ~3 PM\n",
    "        daily_cycle = np.sin(2 * np.pi * (df['hour'] - 5) / 24)\n",
    "        # Seasonal cycle: lowest in January, highest in July\n",
    "        seasonal_cycle = np.sin(2 * np.pi * (df['month'] - 1) / 12)\n",
    "        \n",
    "        # Combine cycles with random noise\n",
    "        temp_base = params['temp_mean']\n",
    "        daily_amplitude = 15  # Daily temperature swing\n",
    "        seasonal_amplitude = params['temp_amplitude']\n",
    "        random_noise = np.random.normal(0, 5, len(df))  # Random fluctuations\n",
    "        \n",
    "        df['temperature'] = (\n",
    "            temp_base +\n",
    "            daily_amplitude * daily_cycle +\n",
    "            seasonal_amplitude * seasonal_cycle +\n",
    "            random_noise\n",
    "        )\n",
    "        \n",
    "        # Generate humidity (%) with inverse relationship to temperature\n",
    "        humid_base = params['humid_mean']\n",
    "        humid_daily = -0.5 * daily_cycle  # Humidity tends to be lower when temperature is higher\n",
    "        humid_seasonal = -0.3 * seasonal_cycle\n",
    "        humid_noise = np.random.normal(0, 10, len(df))\n",
    "        \n",
    "        df['humidity'] = np.clip(\n",
    "            humid_base +\n",
    "            10 * humid_daily +\n",
    "            15 * humid_seasonal +\n",
    "            humid_noise,\n",
    "            10, 100  # Clip to valid humidity range\n",
    "        )\n",
    "        \n",
    "        # Generate precipitation (inches)\n",
    "        # Higher probability in certain months and when humidity is high\n",
    "        rain_prob = params['precip_prob'] * (1 + 0.5 * seasonal_cycle) * (df['humidity'] / 50)\n",
    "        is_raining = np.random.random(len(df)) < rain_prob\n",
    "        \n",
    "        # Amount when raining follows exponential distribution\n",
    "        rain_amount = np.zeros(len(df))\n",
    "        rain_amount[is_raining] = np.random.exponential(0.1, size=is_raining.sum())\n",
    "        \n",
    "        df['precipitation'] = rain_amount\n",
    "        \n",
    "        # Generate wind speed (mph)\n",
    "        wind_base = 5 + 5 * abs(seasonal_cycle)  # More wind in winter and summer\n",
    "        wind_noise = np.random.exponential(3, len(df))\n",
    "        \n",
    "        df['wind_speed'] = wind_base + wind_noise\n",
    "        \n",
    "        # Calculate \"feels like\" temperature based on wind chill and heat index\n",
    "        df['feels_like'] = df['temperature'].copy()\n",
    "        \n",
    "        # Apply wind chill adjustment when cold\n",
    "        cold_mask = df['temperature'] < 50\n",
    "        df.loc[cold_mask, 'feels_like'] = df.loc[cold_mask, 'temperature'] - 0.7 * df.loc[cold_mask, 'wind_speed']\n",
    "        \n",
    "        # Apply heat index adjustment when hot and humid\n",
    "        hot_mask = (df['temperature'] > 80) & (df['humidity'] > 40)\n",
    "        heat_adjustment = 0.1 * (df.loc[hot_mask, 'temperature'] - 80) * (df.loc[hot_mask, 'humidity'] / 100)\n",
    "        df.loc[hot_mask, 'feels_like'] += heat_adjustment\n",
    "        \n",
    "        # Save to cache\n",
    "        cache_file = os.path.join(self.cache_dir, f\"{state.upper()}_{city.title()}_hourly.csv\")\n",
    "        df.to_csv(cache_file, index=False)\n",
    "        \n",
    "        print(f\"Generated {len(df)} hours of sample weather data\")\n",
    "        return df\n",
    "    \n",
    "    def analyze_weather_data(self, city, state):\n",
    "        \"\"\"\n",
    "        Analyze hourly weather data to calculate climate metrics\n",
    "        \n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with weather analysis and risk metrics\n",
    "        \"\"\"\n",
    "        # Get hourly data\n",
    "        df = self.get_hourly_weather_data(city, state)\n",
    "        \n",
    "        # Check if we received a DataFrame or dict\n",
    "        if isinstance(df, dict):\n",
    "            # We got aggregated metrics directly\n",
    "            metrics = df\n",
    "        else:\n",
    "            # Calculate metrics from the DataFrame\n",
    "            metrics = self._calculate_metrics_from_dataframe(df, city, state)\n",
    "        \n",
    "        # Calculate risk score\n",
    "        risk_score, risk_level, risk_factors, positive_factors = self._calculate_risk_score(metrics)\n",
    "        \n",
    "        # Combine everything into analysis result\n",
    "        analysis = {\n",
    "            'city': city,\n",
    "            'state': state,\n",
    "            'metrics': metrics,\n",
    "            'risk_score': risk_score,\n",
    "            'risk_level': risk_level,\n",
    "            'risk_factors': risk_factors,\n",
    "            'positive_factors': positive_factors,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _calculate_metrics_from_dataframe(self, df, city, state):\n",
    "        \"\"\"\n",
    "        Calculate weather metrics from hourly data\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with hourly weather data\n",
    "            city: City name\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with weather metrics\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'city': city,\n",
    "            'state': state,\n",
    "            'data_points': len(df)\n",
    "        }\n",
    "        \n",
    "        # Temperature metrics\n",
    "        if 'temperature' in df.columns:\n",
    "            metrics['temp_avg'] = df['temperature'].mean()\n",
    "            metrics['temp_max'] = df['temperature'].max()\n",
    "            metrics['temp_min'] = df['temperature'].min()\n",
    "            metrics['temp_range'] = metrics['temp_max'] - metrics['temp_min']\n",
    "            \n",
    "            # Temperature extremes\n",
    "            metrics['days_over_90'] = len(df[df['temperature'] > 90]) / 24  # Convert hours to days\n",
    "            metrics['days_over_100'] = len(df[df['temperature'] > 100]) / 24\n",
    "            metrics['days_below_32'] = len(df[df['temperature'] < 32]) / 24\n",
    "            metrics['days_below_20'] = len(df[df['temperature'] < 20]) / 24\n",
    "            \n",
    "            # Temperature variability (standard deviation of daily highs)\n",
    "            if 'date' in df.columns:\n",
    "                daily_highs = df.groupby(df['date'].dt.date)['temperature'].max()\n",
    "                metrics['temp_variability'] = daily_highs.std()\n",
    "        \n",
    "        # Humidity metrics\n",
    "        if 'humidity' in df.columns:\n",
    "            metrics['humidity_avg'] = df['humidity'].mean()\n",
    "            metrics['humidity_max'] = df['humidity'].max()\n",
    "            metrics['humidity_min'] = df['humidity'].min()\n",
    "            \n",
    "            # Extreme humidity\n",
    "            metrics['hours_high_humidity'] = len(df[df['humidity'] > 80])\n",
    "            metrics['percent_high_humidity'] = metrics['hours_high_humidity'] / len(df) * 100\n",
    "        \n",
    "        # Precipitation metrics\n",
    "        if 'precipitation' in df.columns:\n",
    "            metrics['precip_total'] = df['precipitation'].sum()\n",
    "            metrics['precip_max_hourly'] = df['precipitation'].max()\n",
    "            \n",
    "            # Rain days and heavy rain\n",
    "            rain_hours = df[df['precipitation'] > 0]\n",
    "            if 'date' in df.columns:\n",
    "                rain_days = rain_hours['date'].dt.date.nunique()\n",
    "            else:\n",
    "                # Approximate if date format isn't compatible\n",
    "                rain_days = len(rain_hours) / 24  # Rough estimate\n",
    "            \n",
    "            metrics['rain_days'] = rain_days\n",
    "            metrics['heavy_rain_hours'] = len(df[df['precipitation'] > 0.3])  # Hours with >0.3\" rain\n",
    "        \n",
    "        # Wind metrics\n",
    "        if 'wind_speed' in df.columns:\n",
    "            metrics['wind_avg'] = df['wind_speed'].mean()\n",
    "            metrics['wind_max'] = df['wind_speed'].max()\n",
    "            metrics['hours_high_wind'] = len(df[df['wind_speed'] > 20])\n",
    "        \n",
    "        # Comfort metrics\n",
    "        if 'temperature' in df.columns and 'humidity' in df.columns:\n",
    "            # Comfortable conditions: 65-85°F with 30-60% humidity\n",
    "            comfortable = (\n",
    "                (df['temperature'] >= 65) & \n",
    "                (df['temperature'] <= 85) & \n",
    "                (df['humidity'] >= 30) & \n",
    "                (df['humidity'] <= 60)\n",
    "            )\n",
    "            metrics['comfortable_hours'] = len(df[comfortable])\n",
    "            metrics['percent_comfortable'] = metrics['comfortable_hours'] / len(df) * 100\n",
    "            \n",
    "            # Heat index for hours where it applies\n",
    "            hot_humid = (df['temperature'] >= 80) & (df['humidity'] >= 40)\n",
    "            if 'feels_like' in df.columns:\n",
    "                heat_index_diff = df.loc[hot_humid, 'feels_like'] - df.loc[hot_humid, 'temperature']\n",
    "                metrics['avg_heat_index_effect'] = heat_index_diff.mean() if not heat_index_diff.empty else 0\n",
    "        \n",
    "        # Seasonal analysis\n",
    "        if 'date' in df.columns and 'temperature' in df.columns:\n",
    "            # Group by season\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['season'] = pd.cut(\n",
    "                df['month'],\n",
    "                bins=[0, 3, 6, 9, 12],\n",
    "                labels=['Winter', 'Spring', 'Summer', 'Fall'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "            \n",
    "            # Calculate seasonal averages\n",
    "            season_temps = df.groupby('season')['temperature'].mean().to_dict()\n",
    "            metrics['seasonal_temps'] = season_temps\n",
    "            \n",
    "            if 'precipitation' in df.columns:\n",
    "                season_precip = df.groupby('season')['precipitation'].sum().to_dict()\n",
    "                metrics['seasonal_precip'] = season_precip\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_risk_score(self, metrics):\n",
    "        \"\"\"\n",
    "        Calculate weather risk score based on metrics\n",
    "        \n",
    "        Args:\n",
    "            metrics: Dictionary with weather metrics\n",
    "            \n",
    "        Returns:\n",
    "            tuple of (risk_score, risk_level, risk_factors, positive_factors)\n",
    "        \"\"\"\n",
    "        risk_score = 0  # 0-100 scale\n",
    "        risk_factors = []\n",
    "        positive_factors = []\n",
    "        \n",
    "        # Temperature extremes\n",
    "        if 'days_over_90' in metrics:\n",
    "            hot_days = metrics['days_over_90']\n",
    "            if hot_days > 90:  # More than 3 months of 90°+ weather\n",
    "                risk_score += 15\n",
    "                risk_factors.append(f\"Extreme heat: {hot_days:.1f} days above 90°F per year\")\n",
    "            elif hot_days > 45:  # More than 1.5 months of 90°+ weather\n",
    "                risk_score += 10\n",
    "                risk_factors.append(f\"High heat: {hot_days:.1f} days above 90°F per year\")\n",
    "            elif hot_days < 10:\n",
    "                positive_factors.append(\"Minimal extreme heat days\")\n",
    "        \n",
    "        if 'days_below_32' in metrics:\n",
    "            cold_days = metrics['days_below_32']\n",
    "            if cold_days > 90:  # More than 3 months of freezing weather\n",
    "                risk_score += 15\n",
    "                risk_factors.append(f\"Extreme cold: {cold_days:.1f} freezing days per year\")\n",
    "            elif cold_days > 45:\n",
    "                risk_score += 10\n",
    "                risk_factors.append(f\"Cold climate: {cold_days:.1f} freezing days per year\")\n",
    "            elif cold_days < 10:\n",
    "                positive_factors.append(\"Minimal freezing days\")\n",
    "        \n",
    "        # Temperature variability\n",
    "        if 'temp_variability' in metrics:\n",
    "            variability = metrics['temp_variability']\n",
    "            if variability > 15:\n",
    "                risk_score += 10\n",
    "                risk_factors.append(f\"High temperature variability: {variability:.1f}°F std dev\")\n",
    "            elif variability < 8:\n",
    "                positive_factors.append(\"Stable temperature patterns\")\n",
    "        \n",
    "        # Precipitation\n",
    "        if 'precip_total' in metrics:\n",
    "            annual_precip = metrics['precip_total']\n",
    "            if annual_precip > 60:\n",
    "                risk_score += 15\n",
    "                risk_factors.append(f\"Heavy precipitation: {annual_precip:.1f} inches annually\")\n",
    "            elif annual_precip > 45:\n",
    "                risk_score += 10\n",
    "                risk_factors.append(f\"High precipitation: {annual_precip:.1f} inches annually\")\n",
    "            elif annual_precip < 15:\n",
    "                risk_score += 12\n",
    "                risk_factors.append(f\"Drought-prone: Only {annual_precip:.1f} inches annually\")\n",
    "            elif 20 <= annual_precip <= 40:\n",
    "                positive_factors.append(f\"Moderate precipitation: {annual_precip:.1f} inches annually\")\n",
    "        \n",
    "        # Heavy rain events\n",
    "        if 'heavy_rain_hours' in metrics:\n",
    "            heavy_rain = metrics['heavy_rain_hours']\n",
    "            if heavy_rain > 50:\n",
    "                risk_score += 15\n",
    "                risk_factors.append(f\"Flood risk: {heavy_rain} hours of heavy rain annually\")\n",
    "            elif heavy_rain > 25:\n",
    "                risk_score += 10\n",
    "                risk_factors.append(f\"Heavy rainfall events: {heavy_rain} hours annually\")\n",
    "        \n",
    "        # Humidity\n",
    "        if 'percent_high_humidity' in metrics:\n",
    "            high_humidity = metrics['percent_high_humidity']\n",
    "            if high_humidity > 40:\n",
    "                risk_score += 10\n",
    "                risk_factors.append(f\"High humidity: {high_humidity:.1f}% of hours above 80% humidity\")\n",
    "            elif high_humidity < 15:\n",
    "                positive_factors.append(\"Low humidity levels\")\n",
    "        \n",
    "        # Wind\n",
    "        if 'hours_high_wind' in metrics:\n",
    "            high_wind = metrics['hours_high_wind']\n",
    "            if high_wind > 500:  # More than 20 days worth of high wind\n",
    "                risk_score += 12\n",
    "                risk_factors.append(f\"High wind exposure: {high_wind} hours of strong winds annually\")\n",
    "            elif high_wind > 200:\n",
    "                risk_score += 8\n",
    "                risk_factors.append(f\"Moderate wind exposure: {high_wind} hours annually\")\n",
    "        \n",
    "        # Comfort conditions\n",
    "        if 'percent_comfortable' in metrics:\n",
    "            comfort = metrics['percent_comfortable']\n",
    "            if comfort > 60:\n",
    "                positive_factors.append(f\"Excellent comfort: {comfort:.1f}% of hours have ideal conditions\")\n",
    "                risk_score -= 10  # Reduce risk score for very comfortable climates\n",
    "            elif comfort > 40:\n",
    "                positive_factors.append(f\"Good comfort: {comfort:.1f}% of hours have ideal conditions\")\n",
    "                risk_score -= 5\n",
    "            elif comfort < 20:\n",
    "                risk_score += 8\n",
    "                risk_factors.append(f\"Poor comfort: Only {comfort:.1f}% of hours have ideal conditions\")\n",
    "        \n",
    "        # Heat index effect\n",
    "        if 'avg_heat_index_effect' in metrics:\n",
    "            heat_effect = metrics['avg_heat_index_effect']\n",
    "            if heat_effect > 8:\n",
    "                risk_score += 10\n",
    "                risk_factors.append(f\"Dangerous heat index effect: +{heat_effect:.1f}°F average increase\")\n",
    "            elif heat_effect > 5:\n",
    "                risk_score += 5\n",
    "                risk_factors.append(f\"Significant heat index effect: +{heat_effect:.1f}°F average increase\")\n",
    "        \n",
    "        # Seasonal extremes\n",
    "        if 'seasonal_temps' in metrics:\n",
    "            season_temps = metrics['seasonal_temps']\n",
    "            max_season = max(season_temps.items(), key=lambda x: x[1])\n",
    "            min_season = min(season_temps.items(), key=lambda x: x[1])\n",
    "            seasonal_range = max_season[1] - min_season[1]\n",
    "            \n",
    "            if seasonal_range > 40:\n",
    "                risk_score += 10\n",
    "                risk_factors.append(f\"Extreme seasonal variation: {seasonal_range:.1f}°F difference between {min_season[0]} and {max_season[0]}\")\n",
    "            elif seasonal_range < 20:\n",
    "                positive_factors.append(f\"Mild seasonal variation: Only {seasonal_range:.1f}°F difference between seasons\")\n",
    "        \n",
    "        # Determine risk level\n",
    "        if risk_score >= 50:\n",
    "            risk_level = 'high'\n",
    "        elif risk_score >= 30:\n",
    "            risk_level = 'medium'\n",
    "        else:\n",
    "            risk_level = 'low'\n",
    "        \n",
    "        return risk_score, risk_level, risk_factors, positive_factors\n",
    "    \n",
    "    def generate_weather_summary(self, city, state):\n",
    "        \"\"\"\n",
    "        Generate a human-readable summary of weather conditions and risks\n",
    "        \n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            String with formatted weather summary\n",
    "        \"\"\"\n",
    "        # Get analysis\n",
    "        analysis = self.analyze_weather_data(city, state)\n",
    "        metrics = analysis['metrics']\n",
    "        \n",
    "        # Format summary\n",
    "        summary = f\"WEATHER ANALYSIS FOR {city.upper()}, {state.upper()}\\n\"\n",
    "        summary += \"=\" * 50 + \"\\n\\n\"\n",
    "        \n",
    "        # Temperature\n",
    "        summary += \"TEMPERATURE PROFILE:\\n\"\n",
    "        if 'temp_avg' in metrics:\n",
    "            summary += f\"• Average Temperature: {metrics['temp_avg']:.1f}°F\\n\"\n",
    "        if 'temp_max' in metrics and 'temp_min' in metrics:\n",
    "            summary += f\"• Temperature Range: {metrics['temp_min']:.1f}°F to {metrics['temp_max']:.1f}°F\\n\"\n",
    "        if 'days_over_90' in metrics:\n",
    "            summary += f\"• Hot Days: {metrics['days_over_90']:.1f} days above 90°F per year\\n\"\n",
    "        if 'days_below_32' in metrics:\n",
    "            summary += f\"• Freezing Days: {metrics['days_below_32']:.1f} days below 32°F per year\\n\"\n",
    "        \n",
    "        # Precipitation\n",
    "        summary += \"\\nPRECIPITATION PROFILE:\\n\"\n",
    "        if 'precip_total' in metrics:\n",
    "            summary += f\"• Annual Precipitation: {metrics['precip_total']:.1f} inches\\n\"\n",
    "        if 'rain_days' in metrics:\n",
    "            summary += f\"• Rain Days: {metrics['rain_days']:.1f} days per year\\n\"\n",
    "        if 'heavy_rain_hours' in metrics:\n",
    "            summary += f\"• Heavy Rain: {metrics['heavy_rain_hours']} hours of heavy rainfall per year\\n\"\n",
    "        \n",
    "        # Humidity\n",
    "        summary += \"\\nHUMIDITY PROFILE:\\n\"\n",
    "        if 'humidity_avg' in metrics:\n",
    "            summary += f\"• Average Humidity: {metrics['humidity_avg']:.1f}%\\n\"\n",
    "        if 'percent_high_humidity' in metrics:\n",
    "            summary += f\"• High Humidity: {metrics['percent_high_humidity']:.1f}% of hours above 80% humidity\\n\"\n",
    "        \n",
    "        # Comfort metrics\n",
    "        summary += \"\\nCOMFORT METRICS:\\n\"\n",
    "        if 'percent_comfortable' in metrics:\n",
    "            summary += f\"• Comfortable Conditions: {metrics['percent_comfortable']:.1f}% of hours\\n\"\n",
    "        if 'avg_heat_index_effect' in metrics:\n",
    "            summary += f\"• Heat Index Effect: +{metrics['avg_heat_index_effect']:.1f}°F (when applicable)\\n\"\n",
    "        \n",
    "        # Season breakdown\n",
    "        if 'seasonal_temps' in metrics:\n",
    "            summary += \"\\nSEASONAL BREAKDOWN:\\n\"\n",
    "            for season, temp in metrics['seasonal_temps'].items():\n",
    "                precip = metrics.get('seasonal_precip', {}).get(season, 0)\n",
    "                summary += f\"• {season}: {temp:.1f}°F avg, {precip:.1f} inches of precipitation\\n\"\n",
    "        \n",
    "        # Risk assessment\n",
    "        summary += f\"\\nWEATHER RISK ASSESSMENT: {analysis['risk_level'].upper()}\\n\"\n",
    "        summary += f\"Risk Score: {analysis['risk_score']:.1f}/100\\n\\n\"\n",
    "        \n",
    "        if analysis['risk_factors']:\n",
    "            summary += \"Risk Factors:\\n\"\n",
    "            for factor in analysis['risk_factors']:\n",
    "                summary += f\"• {factor}\\n\"\n",
    "            summary += \"\\n\"\n",
    "        \n",
    "        if analysis['positive_factors']:\n",
    "            summary += \"Positive Factors:\\n\"\n",
    "            for factor in analysis['positive_factors']:\n",
    "                summary += f\"• {factor}\\n\"\n",
    "            summary += \"\\n\"\n",
    "        \n",
    "        # Investment implications\n",
    "        summary += \"INVESTMENT IMPLICATIONS:\\n\"\n",
    "        if analysis['risk_level'] == 'high':\n",
    "            summary += \"This location has significant weather-related risks that could impact property values, \"\n",
    "            summary += \"maintenance costs, and insurance rates. Consider these factors carefully in your investment decision.\"\n",
    "        elif analysis['risk_level'] == 'medium':\n",
    "            summary += \"This location has moderate weather-related risks to consider. Some seasonal challenges \"\n",
    "            summary += \"may require additional maintenance or preparation, but they are generally manageable.\"\n",
    "        else:\n",
    "            summary += \"This location has favorable weather conditions for real estate investment. \"\n",
    "            summary += \"The climate presents minimal challenges for property maintenance and livability.\"\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_weather_risk(self, city, state):\n",
    "        \"\"\"\n",
    "        Create a visualization of weather risk analysis\n",
    "        \n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Get analysis\n",
    "        analysis = self.analyze_weather_data(city, state)\n",
    "        metrics = analysis['metrics']\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        fig.suptitle(f\"Weather Risk Analysis for {city}, {state}\", fontsize=16)\n",
    "        \n",
    "        # Temperature plot (top left)\n",
    "        ax1 = axes[0, 0]\n",
    "        if all(key in metrics for key in ['temp_min', 'temp_avg', 'temp_max']):\n",
    "            temp_data = [metrics['temp_min'], metrics['temp_avg'], metrics['temp_max']]\n",
    "            temp_labels = ['Min', 'Avg', 'Max']\n",
    "            \n",
    "            ax1.bar(temp_labels, temp_data, color=['blue', 'green', 'red'])\n",
    "            ax1.set_title('Temperature Profile (°F)')\n",
    "            ax1.set_ylim(min(0, metrics['temp_min'] - 10), metrics['temp_max'] + 10)\n",
    "            \n",
    "            # Add extreme day counts\n",
    "            if 'days_over_90' in metrics and 'days_below_32' in metrics:\n",
    "                ax1.text(0.05, 0.05, \n",
    "                         f\"Days >90°F: {metrics['days_over_90']:.1f}\\nDays <32°F: {metrics['days_below_32']:.1f}\",\n",
    "                         transform=ax1.transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Precipitation plot (top right)\n",
    "        ax2 = axes[0, 1]\n",
    "        if 'seasonal_precip' in metrics:\n",
    "            seasons = list(metrics['seasonal_precip'].keys())\n",
    "            precip_data = list(metrics['seasonal_precip'].values())\n",
    "            \n",
    "            ax2.bar(seasons, precip_data, color='skyblue')\n",
    "            ax2.set_title('Seasonal Precipitation (inches)')\n",
    "            \n",
    "            # Add total precipitation\n",
    "            if 'precip_total' in metrics:\n",
    "                ax2.text(0.05, 0.9, f\"Annual Total: {metrics['precip_total']:.1f} inches\",\n",
    "                         transform=ax2.transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
    "        elif 'precip_total' in metrics:\n",
    "            # Alternative if seasonal data isn't available\n",
    "            ax2.bar(['Annual Total'], [metrics['precip_total']], color='skyblue')\n",
    "            ax2.set_title('Annual Precipitation (inches)')\n",
    "        \n",
    "        # Comfort metrics (bottom left)\n",
    "        ax3 = axes[1, 0]\n",
    "        comfort_metrics = []\n",
    "        comfort_values = []\n",
    "        \n",
    "        # Add available comfort metrics\n",
    "        if 'percent_comfortable' in metrics:\n",
    "            comfort_metrics.append('Comfortable')\n",
    "            comfort_values.append(metrics['percent_comfortable'])\n",
    "        \n",
    "        if 'percent_high_humidity' in metrics:\n",
    "            comfort_metrics.append('High Humidity')\n",
    "            comfort_values.append(metrics['percent_high_humidity'])\n",
    "        \n",
    "        if 'humidity_avg' in metrics:\n",
    "            comfort_metrics.append('Avg Humidity')\n",
    "            comfort_values.append(metrics['humidity_avg'])\n",
    "        \n",
    "        if comfort_metrics:\n",
    "            # Create horizontal bar chart\n",
    "            y_pos = range(len(comfort_metrics))\n",
    "            ax3.barh(y_pos, comfort_values, color=['green', 'orange', 'blue'][:len(comfort_metrics)])\n",
    "            ax3.set_yticks(y_pos)\n",
    "            ax3.set_yticklabels(comfort_metrics)\n",
    "            ax3.set_title('Comfort Metrics (% of time)')\n",
    "            ax3.set_xlim(0, 100)  # Percentage scale\n",
    "            \n",
    "            # Add labels to the bars\n",
    "            for i, v in enumerate(comfort_values):\n",
    "                ax3.text(v + 1, i, f\"{v:.1f}%\", va='center')\n",
    "        \n",
    "        # Risk assessment (bottom right)\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.axis('off')  # No axes for this text panel\n",
    "        \n",
    "        # Set risk level with color\n",
    "        risk_colors = {'high': 'red', 'medium': 'orange', 'low': 'green'}\n",
    "        risk_color = risk_colors.get(analysis['risk_level'], 'gray')\n",
    "        \n",
    "        # Display risk level\n",
    "        ax4.text(0.5, 0.9, f\"WEATHER RISK: {analysis['risk_level'].upper()}\", \n",
    "                 ha='center', fontsize=16, fontweight='bold', color=risk_color,\n",
    "                 transform=ax4.transAxes)\n",
    "        \n",
    "        # Display risk score\n",
    "        ax4.text(0.5, 0.8, f\"Risk Score: {analysis['risk_score']:.1f}/100\", \n",
    "                 ha='center', fontsize=14, transform=ax4.transAxes)\n",
    "        \n",
    "        # List risk factors\n",
    "        if analysis['risk_factors']:\n",
    "            ax4.text(0.5, 0.7, \"Risk Factors:\", ha='center', fontsize=12, \n",
    "                     fontweight='bold', transform=ax4.transAxes)\n",
    "            \n",
    "            for i, factor in enumerate(analysis['risk_factors'][:3]):  # Show top 3\n",
    "                ax4.text(0.5, 0.65 - i*0.07, f\"• {factor}\", ha='center', fontsize=10,\n",
    "                         transform=ax4.transAxes, wrap=True)\n",
    "        \n",
    "        # List positive factors\n",
    "        if analysis['positive_factors']:\n",
    "            pos_y = 0.7 - (len(analysis['risk_factors'][:3]) * 0.07) - 0.1\n",
    "            ax4.text(0.5, pos_y, \"Positive Factors:\", ha='center', fontsize=12,\n",
    "                     fontweight='bold', color='green', transform=ax4.transAxes)\n",
    "            \n",
    "            for i, factor in enumerate(analysis['positive_factors'][:2]):  # Show top 2\n",
    "                ax4.text(0.5, pos_y - 0.07 - i*0.07, f\"• {factor}\", ha='center', \n",
    "                         fontsize=10, color='green', transform=ax4.transAxes, wrap=True)\n",
    "        \n",
    "        # Add timestamp\n",
    "        fig.text(0.95, 0.01, f\"Generated: {analysis['timestamp']}\", \n",
    "                 ha='right', fontsize=8, color='gray')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def generate_weather_metrics_for_prediction(self, city, state):\n",
    "        \"\"\"\n",
    "        Generate weather metrics specifically formatted for price prediction models\n",
    "        \n",
    "        Args:\n",
    "            city: City name\n",
    "            state: State code\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with weather metrics suitable for prediction models\n",
    "        \"\"\"\n",
    "        # Get full analysis\n",
    "        analysis = self.analyze_weather_data(city, state)\n",
    "        metrics = analysis['metrics']\n",
    "        \n",
    "        # Create a simplified set of metrics for prediction models\n",
    "        prediction_metrics = {\n",
    "            'weather_risk_score': analysis['risk_score'],\n",
    "            'weather_risk_level': analysis['risk_level'],\n",
    "        }\n",
    "        \n",
    "        # Add key metrics that would be useful for prediction\n",
    "        if 'temp_avg' in metrics:\n",
    "            prediction_metrics['avg_temperature'] = metrics['temp_avg']\n",
    "        \n",
    "        if 'precip_total' in metrics:\n",
    "            prediction_metrics['annual_precipitation'] = metrics['precip_total']\n",
    "        \n",
    "        if 'days_over_90' in metrics:\n",
    "            prediction_metrics['extreme_heat_days'] = metrics['days_over_90']\n",
    "        \n",
    "        if 'days_below_32' in metrics:\n",
    "            prediction_metrics['freezing_days'] = metrics['days_below_32']\n",
    "        \n",
    "        if 'percent_comfortable' in metrics:\n",
    "            prediction_metrics['comfort_index'] = metrics['percent_comfortable']\n",
    "        \n",
    "        if 'heavy_rain_hours' in metrics:\n",
    "            prediction_metrics['heavy_rain_frequency'] = metrics['heavy_rain_hours']\n",
    "        \n",
    "        # Add normalized risk scores (0-1 scale) for key factors\n",
    "        # These can be directly used as features in prediction models\n",
    "        prediction_metrics['temp_risk'] = min(1.0, metrics.get('days_over_90', 0) / 120 + metrics.get('days_below_32', 0) / 120)\n",
    "        prediction_metrics['precip_risk'] = min(1.0, abs(metrics.get('precip_total', 30) - 30) / 30)\n",
    "        prediction_metrics['comfort_risk'] = max(0.0, 1.0 - metrics.get('percent_comfortable', 50) / 100)\n",
    "        \n",
    "        return prediction_metrics\n",
    "\n",
    "\n",
    "# Test functionality\n",
    "if __name__ == \"__main__\":\n",
    "    # Create analyzer\n",
    "    analyzer = HourlyWeatherAnalyzer()\n",
    "    \n",
    "    # Set API token if available (not required for sample data)\n",
    "    api_token = \"HrLmaYzJYcXfoXZvFJiKvOzTTJLLvBDv\"  # Replace with your actual token\n",
    "    analyzer.set_api_token(api_token)\n",
    "    \n",
    "    # Test with a sample location\n",
    "    test_city = \"Austin\"\n",
    "    test_state = \"TX\"\n",
    "    \n",
    "    try:\n",
    "        # Get hourly weather data (will use sample data if API fails)\n",
    "        print(f\"Analyzing weather data for {test_city}, {test_state}...\")\n",
    "        weather_analysis = analyzer.analyze_weather_data(test_city, test_state)\n",
    "        \n",
    "        # Print risk assessment\n",
    "        print(f\"\\nWeather Risk Assessment: {weather_analysis['risk_level'].upper()}\")\n",
    "        print(f\"Risk Score: {weather_analysis['risk_score']:.1f}/100\")\n",
    "        \n",
    "        # Print risk factors\n",
    "        print(\"\\nRisk Factors:\")\n",
    "        for factor in weather_analysis['risk_factors']:\n",
    "            print(f\"- {factor}\")\n",
    "        \n",
    "        # Print positive factors\n",
    "        print(\"\\nPositive Factors:\")\n",
    "        for factor in weather_analysis['positive_factors']:\n",
    "            print(f\"- {factor}\")\n",
    "        \n",
    "        # Generate and print weather summary\n",
    "        summary = analyzer.generate_weather_summary(test_city, test_state)\n",
    "        print(\"\\n\" + summary)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = analyzer.plot_weather_risk(test_city, test_state)\n",
    "        plt.savefig(f\"{test_state}_{test_city}_weather_risk.png\")\n",
    "        plt.close(fig)\n",
    "        print(f\"\\nSaved visualization to {test_state}_{test_city}_weather_risk.png\")\n",
    "        \n",
    "        # Get prediction metrics\n",
    "        prediction_metrics = analyzer.generate_weather_metrics_for_prediction(test_city, test_state)\n",
    "        print(\"\\nPrediction Metrics:\")\n",
    "        for key, value in prediction_metrics.items():\n",
    "            print(f\"- {key}: {value}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in testing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6726f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dca5c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded merged data: 5812149 rows, 57 columns\n",
      "Loaded FEMA data: 20000 rows, 9 columns\n",
      "{'total_disasters': 14672005.0, 'yearly_avg': 2445334.1666666665, 'risk_level': 'high'}\n"
     ]
    }
   ],
   "source": [
    "merged = processor.load_merged_data(\"/Users/shivangi/Downloads/merged_fema_redfin_dataset.csv\")\n",
    "fema = processor.load_fema_data(\"/Users/shivangi/Downloads/fema_cleaned.csv\")\n",
    "summary = processor.get_state_disaster_summary(\"CALIFORNIA\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ac0b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTICUT has 69155 records in the merged dataset.\n"
     ]
    }
   ],
   "source": [
    "# Count records for ALABAMA\n",
    "alabama_count = processor.merged_df[processor.merged_df['STATE'] == 'CONNECTICUT'].shape[0]\n",
    "print(f\"CONNECTICUT has {alabama_count} records in the merged dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fea4250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 69155 records for CONNECTICUT to:\n",
      "/Users/shivangi/Downloads/CONNECTICUT_real_estate_data.xlsx ✅\n"
     ]
    }
   ],
   "source": [
    "# Filter all records where STATE is CALIFORNIA\n",
    "california_df = processor.merged_df[processor.merged_df['STATE'] == 'CONNECTICUT']\n",
    "\n",
    "# Save to Excel\n",
    "output_path = \"/Users/shivangi/Downloads/CONNECTICUT_real_estate_data.xlsx\"\n",
    "california_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(california_df)} records for CONNECTICUT to:\\n{output_path} ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10d4988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File saved to ~/Downloads/redfin_with_disaster_score.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge logic (same as you have)\n",
    "merged_df = pd.read_csv(\"/Users/shivangi/Downloads/merged_fema_redfin_dataset.csv\")\n",
    "disaster_scores = pd.read_csv(\"/Users/shivangi/Downloads/City_Natural_Disaster_Score__0_30_realistic_.csv\")\n",
    "\n",
    "merged_df['CITY'] = merged_df['CITY'].astype(str).str.title().str.strip()\n",
    "merged_df['STATE'] = merged_df['STATE'].astype(str).str.upper().str.strip()\n",
    "disaster_scores['CITY'] = disaster_scores['CITY'].astype(str).str.title().str.strip()\n",
    "disaster_scores['STATE'] = disaster_scores['STATE'].astype(str).str.upper().str.strip()\n",
    "\n",
    "merged_result = pd.merge(merged_df, disaster_scores, on=['STATE', 'CITY'], how='left')\n",
    "\n",
    "# ✅ Save to Downloads\n",
    "merged_result.to_csv(\"/Users/shivangi/Downloads/redfin_with_disaster_score.csv\", index=False)\n",
    "\n",
    "print(\"✅ File saved to ~/Downloads/redfin_with_disaster_score.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a385df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Top 1000 rows with disaster scores saved to: redfin_with_disaster_score_top1000.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Get top 1000 rows\n",
    "top_1000 = merged_result.head(1000)\n",
    "\n",
    "# Step 6: Save to CSV in current directory\n",
    "output_file = \"redfin_with_disaster_score_top1000.csv\"\n",
    "top_1000.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Top 1000 rows with disaster scores saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ef857ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>CITY</th>\n",
       "      <th>natural_disaster_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Abanda</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Adamsville</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Akron</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Alabaster</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22946</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Wamsutter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22947</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Washam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22948</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22949</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Woods Landing-Jelm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22950</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>Wright</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22951 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         STATE                CITY  natural_disaster_score\n",
       "0      ALABAMA              Abanda                      22\n",
       "1      ALABAMA           Abbeville                      19\n",
       "2      ALABAMA          Adamsville                      20\n",
       "3      ALABAMA               Akron                      22\n",
       "4      ALABAMA           Alabaster                      18\n",
       "...        ...                 ...                     ...\n",
       "22946  WYOMING           Wamsutter                       1\n",
       "22947  WYOMING              Washam                       1\n",
       "22948  WYOMING              Wilson                       3\n",
       "22949  WYOMING  Woods Landing-Jelm                       5\n",
       "22950  WYOMING              Wright                       4\n",
       "\n",
       "[22951 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load city disaster data\n",
    "city_disaster = pd.read_csv('/Users/shivangi/Downloads/City_Natural_Disaster_Score__0_30_realistic_.csv')\n",
    "city_disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f2a9506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4591/4591 [08:03<00:00,  9.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load city disaster data\n",
    "city_disaster = pd.read_csv('/Users/shivangi/Downloads/City_Natural_Disaster_Score__0_30_realistic_.csv')\n",
    "unique_cities = city_disaster[['STATE', 'CITY', 'natural_disaster_score']].drop_duplicates()\n",
    "\n",
    "# Set date range: past three years, daily\n",
    "start_date = datetime(2022, 5, 3)\n",
    "end_date = datetime(2025, 5, 2)  # inclusive\n",
    "total_days = (end_date - start_date).days + 1\n",
    "timestamps = [start_date + timedelta(days=d) for d in range(total_days)]\n",
    "\n",
    "def generate_weather(city, state, disaster_score):\n",
    "    # Base values influenced by disaster score\n",
    "    base_temp = 70 - disaster_score * 0.7\n",
    "    temp_range = 20 + disaster_score * 0.5\n",
    "    base_wind = 8 + disaster_score * 0.3\n",
    "    base_precip = disaster_score * 0.08 * 24  # Convert hourly to daily total\n",
    "    base_humidity = 60 + disaster_score * 0.6\n",
    "\n",
    "    city_data = []\n",
    "    for ts in timestamps:\n",
    "        day_of_year = ts.timetuple().tm_yday\n",
    "        # Seasonal variation\n",
    "        season_adj = 10 * np.sin((2 * np.pi * (day_of_year - 172)) / 365.25)\n",
    "        # Daily temp (average for the day)\n",
    "        temp = base_temp + temp_range * 0.5 + season_adj + np.random.normal(0, 1.5)\n",
    "        \n",
    "        city_data.append({\n",
    "            'state': state,\n",
    "            'city': city,\n",
    "            'date': ts.date(),\n",
    "            'avg_temp': round(temp, 1),\n",
    "            'min_temp': round(temp - np.random.uniform(2,4), 1),\n",
    "            'max_temp': round(temp + np.random.uniform(2,4), 1),\n",
    "            'wind_speed': round(base_wind + np.random.normal(0, 1.2), 1),\n",
    "            'precipitation': round(max(0, base_precip + np.random.normal(0, 0.5)), 2),\n",
    "            'humidity': round(base_humidity + np.random.normal(0, 5), 1),\n",
    "            'pressure': round(1013 + np.random.normal(0, 2), 1),\n",
    "            'disaster_score': disaster_score\n",
    "        })\n",
    "    return city_data\n",
    "\n",
    "# Generate and save data in chunks\n",
    "chunk_size = 5  # Adjust based on memory\n",
    "all_city_indices = list(unique_cities.index)\n",
    "\n",
    "for i in tqdm(range(0, len(all_city_indices), chunk_size)):\n",
    "    chunk_indices = all_city_indices[i:i+chunk_size]\n",
    "    weather_data = []\n",
    "    for idx in chunk_indices:\n",
    "        row = unique_cities.loc[idx]\n",
    "        weather_data.extend(generate_weather(row['CITY'], row['STATE'], row['natural_disaster_score']))\n",
    "    \n",
    "    weather_df = pd.DataFrame(weather_data)\n",
    "    if i == 0:\n",
    "        weather_df.to_csv('synthetic_noaa_daily_precipitation.csv', index=False, mode='w')\n",
    "    else:\n",
    "        weather_df.to_csv('synthetic_noaa_daily_precipitation.csv', index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e014b526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>date</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>disaster_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Abanda</td>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>60.9</td>\n",
       "      <td>58.5</td>\n",
       "      <td>63.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>42.39</td>\n",
       "      <td>78.5</td>\n",
       "      <td>1016.8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Abanda</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>64.6</td>\n",
       "      <td>62.3</td>\n",
       "      <td>67.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>42.81</td>\n",
       "      <td>78.6</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Abanda</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>61.7</td>\n",
       "      <td>57.7</td>\n",
       "      <td>64.8</td>\n",
       "      <td>13.9</td>\n",
       "      <td>41.49</td>\n",
       "      <td>75.5</td>\n",
       "      <td>1016.5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Abanda</td>\n",
       "      <td>2022-05-06</td>\n",
       "      <td>65.9</td>\n",
       "      <td>63.4</td>\n",
       "      <td>69.3</td>\n",
       "      <td>13.6</td>\n",
       "      <td>41.81</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1011.6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>Abanda</td>\n",
       "      <td>2022-05-07</td>\n",
       "      <td>61.5</td>\n",
       "      <td>59.1</td>\n",
       "      <td>65.1</td>\n",
       "      <td>14.6</td>\n",
       "      <td>42.62</td>\n",
       "      <td>78.6</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state    city        date  avg_temp  min_temp  max_temp  wind_speed  \\\n",
       "0  ALABAMA  Abanda  2022-05-03      60.9      58.5      63.0        14.0   \n",
       "1  ALABAMA  Abanda  2022-05-04      64.6      62.3      67.5        15.5   \n",
       "2  ALABAMA  Abanda  2022-05-05      61.7      57.7      64.8        13.9   \n",
       "3  ALABAMA  Abanda  2022-05-06      65.9      63.4      69.3        13.6   \n",
       "4  ALABAMA  Abanda  2022-05-07      61.5      59.1      65.1        14.6   \n",
       "\n",
       "   precipitation  humidity  pressure  disaster_score  \n",
       "0          42.39      78.5    1016.8              22  \n",
       "1          42.81      78.6    1009.7              22  \n",
       "2          41.49      75.5    1016.5              22  \n",
       "3          41.81      71.0    1011.6              22  \n",
       "4          42.62      78.6    1014.2              22  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_disaster = pd.read_csv('/Users/shivangi/Desktop/Project/synthetic_noaa_daily_precipitation.csv')\n",
    "ppt_disaster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a458221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA monthly shape: (849187, 13)\n",
      "     STATE    CITY  YEAR  MONTH   avg_temp   min_temp   max_temp  wind_speed  \\\n",
      "0  ALABAMA  Abanda  2022      5  64.324138  61.320690  67.362069   14.648276   \n",
      "1  ALABAMA  Abanda  2022      6  69.410000  66.466667  72.516667   14.603333   \n",
      "2  ALABAMA  Abanda  2022      7  74.535484  71.532258  77.441935   14.803226   \n",
      "3  ALABAMA  Abanda  2022      8  77.819355  74.929032  80.832258   14.838710   \n",
      "4  ALABAMA  Abanda  2022      9  80.203333  77.206667  83.086667   14.620000   \n",
      "\n",
      "   precipitation   humidity     pressure  disaster_score  source_count  \n",
      "0        1227.26  73.986207  1013.110345            22.0            29  \n",
      "1        1264.88  74.396667  1013.733333            22.0            30  \n",
      "2        1310.22  72.412903  1013.277419            22.0            31  \n",
      "3        1307.60  74.219355  1013.570968            22.0            31  \n",
      "4        1266.00  74.656667  1013.180000            22.0            30  \n",
      "Redfin shape: (5812149, 59)\n",
      "Merged data sample:\n",
      "           CITY           STATE  YEAR  MONTH  MEDIAN_SALE_PRICE   avg_temp  \\\n",
      "0  Indian Trail  NORTH CAROLINA  2015     10           223000.0        NaN   \n",
      "1   Hainesville        ILLINOIS  2014      4            82625.0        NaN   \n",
      "2     Maysville  NORTH CAROLINA  2017     10            67256.0        NaN   \n",
      "3      Chewelah      WASHINGTON  2024      7           317500.0  80.354839   \n",
      "4       Lombard        ILLINOIS  2022      1           340000.0        NaN   \n",
      "\n",
      "   disaster_score  source_count  \n",
      "0             NaN           NaN  \n",
      "1             NaN           NaN  \n",
      "2             NaN           NaN  \n",
      "3             9.0          31.0  \n",
      "4             NaN           NaN  \n",
      "✅ Merged dataset saved to redfin_with_fema_noaa.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load NOAA daily data\n",
    "noaa_df = pd.read_csv(\"/Users/shivangi/Desktop/Project/synthetic_noaa_daily_precipitation.csv\")\n",
    "noaa_df['date'] = pd.to_datetime(noaa_df['date'], errors='coerce')\n",
    "noaa_df['YEAR'] = noaa_df['date'].dt.year\n",
    "noaa_df['MONTH'] = noaa_df['date'].dt.month\n",
    "noaa_df['STATE'] = noaa_df['state'].str.upper().str.strip()\n",
    "noaa_df['CITY'] = noaa_df['city'].str.title().str.strip()\n",
    "\n",
    "# Step 2: Aggregate NOAA to monthly per city-state\n",
    "noaa_monthly = noaa_df.groupby(['STATE', 'CITY', 'YEAR', 'MONTH']).agg({\n",
    "    'avg_temp': 'mean',\n",
    "    'min_temp': 'mean',\n",
    "    'max_temp': 'mean',\n",
    "    'wind_speed': 'mean',\n",
    "    'precipitation': 'sum',\n",
    "    'humidity': 'mean',\n",
    "    'pressure': 'mean',\n",
    "    'disaster_score': 'mean',\n",
    "    'date': 'count'  # number of records contributing\n",
    "}).reset_index().rename(columns={'date': 'source_count'})\n",
    "\n",
    "print(f\"NOAA monthly shape: {noaa_monthly.shape}\")\n",
    "print(noaa_monthly.head())\n",
    "\n",
    "# Step 3: Load Redfin monthly data\n",
    "redfin_df = pd.read_csv(\"redfin_with_disaster_score.csv\")\n",
    "redfin_df['PERIOD_BEGIN'] = pd.to_datetime(redfin_df['PERIOD_BEGIN'], errors='coerce')\n",
    "redfin_df['YEAR'] = redfin_df['PERIOD_BEGIN'].dt.year\n",
    "redfin_df['MONTH'] = redfin_df['PERIOD_BEGIN'].dt.month\n",
    "redfin_df['STATE'] = redfin_df['STATE'].str.upper().str.strip()\n",
    "redfin_df['CITY'] = redfin_df['CITY'].str.title().str.strip()\n",
    "\n",
    "print(f\"Redfin shape: {redfin_df.shape}\")\n",
    "\n",
    "# Step 4: Merge Redfin with NOAA monthly data\n",
    "merged_df = pd.merge(\n",
    "    redfin_df,\n",
    "    noaa_monthly,\n",
    "    on=['STATE', 'CITY', 'YEAR', 'MONTH'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Merged data sample:\")\n",
    "print(merged_df[['CITY', 'STATE', 'YEAR', 'MONTH', 'MEDIAN_SALE_PRICE', 'avg_temp', 'disaster_score', 'source_count']].head())\n",
    "\n",
    "# Step 5: Save output\n",
    "merged_df.to_csv(\"/Users/shivangi/Downloads/redfin_with_fema_noaa.csv\", index=False)\n",
    "print(\"✅ Merged dataset saved to redfin_with_fema_noaa.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b4f1a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 5812149\n",
      "Rows missing weather data: 4488408 (77.22%)\n",
      "\n",
      "Missing weather data by state (top 5):\n",
      "STATE\n",
      "CALIFORNIA      365423\n",
      "NEW YORK        334401\n",
      "PENNSYLVANIA    311991\n",
      "FLORIDA         282490\n",
      "ILLINOIS        218891\n",
      "dtype: int64\n",
      "\n",
      "Missing weather data by year:\n",
      "YEAR\n",
      "2022    155064\n",
      "2012    342681\n",
      "2013    369510\n",
      "2014    396069\n",
      "2015    433551\n",
      "2016    450540\n",
      "2017    457520\n",
      "2018    459413\n",
      "2019    463513\n",
      "2020    469833\n",
      "2021    490714\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check percentage of rows with missing weather data\n",
    "missing_weather = merged_df['avg_temp'].isna().sum()\n",
    "total_rows = len(merged_df)\n",
    "missing_pct = (missing_weather / total_rows) * 100\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows missing weather data: {missing_weather} ({missing_pct:.2f}%)\")\n",
    "\n",
    "# Check if missing data is evenly distributed across states/years\n",
    "missing_by_state = merged_df[merged_df['avg_temp'].isna()].groupby('STATE').size()\n",
    "missing_by_year = merged_df[merged_df['avg_temp'].isna()].groupby('YEAR').size()\n",
    "\n",
    "print(\"\\nMissing weather data by state (top 5):\")\n",
    "print(missing_by_state.sort_values(ascending=False).head())\n",
    "\n",
    "print(\"\\nMissing weather data by year:\")\n",
    "print(missing_by_year.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3145e6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simple weather data imputation...\n",
      "Missing values before: {'avg_temp': 4488408, 'min_temp': 4488408, 'max_temp': 4488408, 'wind_speed': 4488408, 'precipitation': 4488408, 'humidity': 4488408, 'pressure': 4488408, 'disaster_score': 4488408}\n",
      "Calculating month-year medians...\n",
      "Missing values after month-year filling: {'avg_temp': 4488408, 'min_temp': 4488408, 'max_temp': 4488408, 'wind_speed': 4488408, 'precipitation': 4488408, 'humidity': 4488408, 'pressure': 4488408, 'disaster_score': 4488408}\n",
      "Some month-years have no data. Filling remaining missing values with overall medians...\n",
      "Final missing values: {'avg_temp': 0, 'min_temp': 0, 'max_temp': 0, 'wind_speed': 0, 'precipitation': 0, 'humidity': 0, 'pressure': 0, 'disaster_score': 0}\n",
      "Filled weather data for 4488408 rows (77.22% of dataset)\n",
      "Weather data filling complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fill_weather_data_simple(merged_df):\n",
    "    \"\"\"\n",
    "    Fill missing weather data with month-year medians.\n",
    "    This uses one straightforward approach: fill all missing values for a given\n",
    "    month and year with the median values from available data for that same month and year.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    merged_df : pandas DataFrame\n",
    "        The merged Redfin and NOAA dataset with missing weather values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The dataset with filled weather values\n",
    "    \"\"\"\n",
    "    print(\"Starting simple weather data imputation...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = merged_df.copy()\n",
    "    \n",
    "    # Identify weather columns that need filling\n",
    "    weather_cols = ['avg_temp', 'min_temp', 'max_temp', 'wind_speed', \n",
    "                   'precipitation', 'humidity', 'pressure', 'disaster_score']\n",
    "    \n",
    "    # Count missing values before filling\n",
    "    missing_before = {col: df[col].isna().sum() for col in weather_cols}\n",
    "    print(f\"Missing values before: {missing_before}\")\n",
    "    \n",
    "    # Calculate medians for each month-year combination\n",
    "    print(\"Calculating month-year medians...\")\n",
    "    month_year_medians = df.groupby(['YEAR', 'MONTH'])[weather_cols].transform('median')\n",
    "    \n",
    "    # Fill missing values with these medians\n",
    "    for col in weather_cols:\n",
    "        df[col] = df[col].fillna(month_year_medians[col])\n",
    "    \n",
    "    # Count missing values after month-year filling\n",
    "    missing_after_month_year = {col: df[col].isna().sum() for col in weather_cols}\n",
    "    print(f\"Missing values after month-year filling: {missing_after_month_year}\")\n",
    "    \n",
    "    # If any values are still missing, fill with overall median\n",
    "    if any(count > 0 for count in missing_after_month_year.values()):\n",
    "        print(\"Some month-years have no data. Filling remaining missing values with overall medians...\")\n",
    "        for col in weather_cols:\n",
    "            if df[col].isna().sum() > 0:\n",
    "                overall_median = df[col].median()\n",
    "                df[col] = df[col].fillna(overall_median)\n",
    "    \n",
    "    # Count final missing values\n",
    "    missing_final = {col: df[col].isna().sum() for col in weather_cols}\n",
    "    print(f\"Final missing values: {missing_final}\")\n",
    "    \n",
    "    # Add a flag to indicate which rows had their weather data filled\n",
    "    df['weather_data_filled'] = False\n",
    "    original_mask = merged_df['avg_temp'].notna()\n",
    "    df.loc[original_mask, 'weather_data_filled'] = False\n",
    "    df.loc[~original_mask, 'weather_data_filled'] = True\n",
    "    \n",
    "    filled_count = (~original_mask).sum()\n",
    "    print(f\"Filled weather data for {filled_count} rows ({filled_count/len(df)*100:.2f}% of dataset)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the merged data\n",
    "    merged_df = pd.read_csv(\"/Users/shivangi/Downloads/redfin_with_fema_noaa.csv\")\n",
    "    \n",
    "    # Fill missing weather data\n",
    "    filled_df = fill_weather_data_simple(merged_df)\n",
    "    \n",
    "    # Save the results\n",
    "    filled_df.to_csv(\"/Users/shivangi/Downloads/filled_redfin_noaa_data.csv\", index=False)\n",
    "    \n",
    "    print(\"Weather data filling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a929c56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD_BEGIN</th>\n",
       "      <th>PERIOD_END</th>\n",
       "      <th>PERIOD_DURATION</th>\n",
       "      <th>REGION_TYPE</th>\n",
       "      <th>IS_SEASONALLY_ADJUSTED</th>\n",
       "      <th>REGION</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_CODE</th>\n",
       "      <th>PROPERTY_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>disaster_score</th>\n",
       "      <th>source_count</th>\n",
       "      <th>weather_data_filled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>2015-10-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Indian Trail, NC</td>\n",
       "      <td>Indian Trail</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>NC</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>2014-04-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Hainesville, IL</td>\n",
       "      <td>Hainesville</td>\n",
       "      <td>ILLINOIS</td>\n",
       "      <td>IL</td>\n",
       "      <td>Condo/Co-op</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Maysville, NC</td>\n",
       "      <td>Maysville</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>NC</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Chewelah, WA</td>\n",
       "      <td>Chewelah</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>WA</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>80.354839</td>\n",
       "      <td>77.393548</td>\n",
       "      <td>83.332258</td>\n",
       "      <td>10.232258</td>\n",
       "      <td>539.54</td>\n",
       "      <td>64.106452</td>\n",
       "      <td>1013.070968</td>\n",
       "      <td>9.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Lombard, IL</td>\n",
       "      <td>Lombard</td>\n",
       "      <td>ILLINOIS</td>\n",
       "      <td>IL</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Augusta-Richmond County consolidated governmen...</td>\n",
       "      <td>Augusta-Richmond County Consolidated Governmen...</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>GA</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Concord, NC</td>\n",
       "      <td>Concord</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>NC</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Eldersburg, MD</td>\n",
       "      <td>Eldersburg</td>\n",
       "      <td>MARYLAND</td>\n",
       "      <td>MD</td>\n",
       "      <td>Condo/Co-op</td>\n",
       "      <td>...</td>\n",
       "      <td>64.609677</td>\n",
       "      <td>61.400000</td>\n",
       "      <td>67.477419</td>\n",
       "      <td>11.461290</td>\n",
       "      <td>716.18</td>\n",
       "      <td>66.877419</td>\n",
       "      <td>1013.167742</td>\n",
       "      <td>12.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>2013-08-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Laplace, LA</td>\n",
       "      <td>Laplace</td>\n",
       "      <td>LOUISIANA</td>\n",
       "      <td>LA</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Egypt Lake-Leto, FL</td>\n",
       "      <td>Egypt Lake-Leto</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>FL</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Dupo, IL</td>\n",
       "      <td>Dupo</td>\n",
       "      <td>ILLINOIS</td>\n",
       "      <td>IL</td>\n",
       "      <td>Multi-Family (2-4 Unit)</td>\n",
       "      <td>...</td>\n",
       "      <td>73.853333</td>\n",
       "      <td>70.796667</td>\n",
       "      <td>76.860000</td>\n",
       "      <td>11.223333</td>\n",
       "      <td>631.52</td>\n",
       "      <td>66.506667</td>\n",
       "      <td>1013.066667</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Sea Ranch Lakes, FL</td>\n",
       "      <td>Sea Ranch Lakes</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>FL</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Monroeville, OH</td>\n",
       "      <td>Monroeville</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>OH</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Chevy Chase Village, MD</td>\n",
       "      <td>Chevy Chase Village</td>\n",
       "      <td>MARYLAND</td>\n",
       "      <td>MD</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Newburyport, MA</td>\n",
       "      <td>Newburyport</td>\n",
       "      <td>MASSACHUSETTS</td>\n",
       "      <td>MA</td>\n",
       "      <td>Condo/Co-op</td>\n",
       "      <td>...</td>\n",
       "      <td>83.667742</td>\n",
       "      <td>80.909677</td>\n",
       "      <td>86.545161</td>\n",
       "      <td>11.196774</td>\n",
       "      <td>657.50</td>\n",
       "      <td>65.558065</td>\n",
       "      <td>1012.887097</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Meridian, PA</td>\n",
       "      <td>Meridian</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>PA</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>76.264516</td>\n",
       "      <td>73.219355</td>\n",
       "      <td>79.454839</td>\n",
       "      <td>11.432258</td>\n",
       "      <td>659.08</td>\n",
       "      <td>68.280645</td>\n",
       "      <td>1012.406452</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Yorba Linda, CA</td>\n",
       "      <td>Yorba Linda</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>CA</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>2015-09-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Elkhart, IN</td>\n",
       "      <td>Elkhart</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>IN</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>2024-04-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Salisbury, NC</td>\n",
       "      <td>Salisbury</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>NC</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>61.553333</td>\n",
       "      <td>58.540000</td>\n",
       "      <td>64.396667</td>\n",
       "      <td>14.373333</td>\n",
       "      <td>1268.27</td>\n",
       "      <td>73.600000</td>\n",
       "      <td>1013.456667</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Wellford, SC</td>\n",
       "      <td>Wellford</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>SC</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunray, OK</td>\n",
       "      <td>Sunray</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>OK</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>66.270968</td>\n",
       "      <td>63.287097</td>\n",
       "      <td>69.241935</td>\n",
       "      <td>13.325806</td>\n",
       "      <td>1073.85</td>\n",
       "      <td>70.467742</td>\n",
       "      <td>1012.890323</td>\n",
       "      <td>18.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Hudson, WI</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>WISCONSIN</td>\n",
       "      <td>WI</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Rossville, MD</td>\n",
       "      <td>Rossville</td>\n",
       "      <td>MARYLAND</td>\n",
       "      <td>MD</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>...</td>\n",
       "      <td>78.764516</td>\n",
       "      <td>75.603226</td>\n",
       "      <td>81.812903</td>\n",
       "      <td>11.609677</td>\n",
       "      <td>714.24</td>\n",
       "      <td>67.829032</td>\n",
       "      <td>1012.741935</td>\n",
       "      <td>12.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>2014-10-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Wilmore, KY</td>\n",
       "      <td>Wilmore</td>\n",
       "      <td>KENTUCKY</td>\n",
       "      <td>KY</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Van Bibber Lake, IN</td>\n",
       "      <td>Van Bibber Lake</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>IN</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.132258</td>\n",
       "      <td>71.203226</td>\n",
       "      <td>77.109677</td>\n",
       "      <td>11.987097</td>\n",
       "      <td>831.38</td>\n",
       "      <td>68.670968</td>\n",
       "      <td>1012.641935</td>\n",
       "      <td>14.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Troy, IL</td>\n",
       "      <td>Troy</td>\n",
       "      <td>ILLINOIS</td>\n",
       "      <td>IL</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Osseo, MN</td>\n",
       "      <td>Osseo</td>\n",
       "      <td>MINNESOTA</td>\n",
       "      <td>MN</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>...</td>\n",
       "      <td>77.835484</td>\n",
       "      <td>74.712903</td>\n",
       "      <td>80.777419</td>\n",
       "      <td>10.261290</td>\n",
       "      <td>413.62</td>\n",
       "      <td>64.470968</td>\n",
       "      <td>1012.896774</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Spring House, PA</td>\n",
       "      <td>Spring House</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>PA</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Polo, MO</td>\n",
       "      <td>Polo</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>MO</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Edgartown, MA</td>\n",
       "      <td>Edgartown</td>\n",
       "      <td>MASSACHUSETTS</td>\n",
       "      <td>MA</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Damascus, OR</td>\n",
       "      <td>Damascus</td>\n",
       "      <td>OREGON</td>\n",
       "      <td>OR</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Indian Lake, PA</td>\n",
       "      <td>Indian Lake</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>PA</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2014-03-01</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Saline, MI</td>\n",
       "      <td>Saline</td>\n",
       "      <td>MICHIGAN</td>\n",
       "      <td>MI</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>2024-05-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Manchester, CT</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>CONNECTICUT</td>\n",
       "      <td>CT</td>\n",
       "      <td>Multi-Family (2-4 Unit)</td>\n",
       "      <td>...</td>\n",
       "      <td>70.880645</td>\n",
       "      <td>67.822581</td>\n",
       "      <td>74.019355</td>\n",
       "      <td>10.267742</td>\n",
       "      <td>418.80</td>\n",
       "      <td>63.225806</td>\n",
       "      <td>1012.996774</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Selma, CA</td>\n",
       "      <td>Selma</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>CA</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Keats, KS</td>\n",
       "      <td>Keats</td>\n",
       "      <td>KANSAS</td>\n",
       "      <td>KS</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>81.483333</td>\n",
       "      <td>78.593333</td>\n",
       "      <td>84.393333</td>\n",
       "      <td>10.610000</td>\n",
       "      <td>463.92</td>\n",
       "      <td>65.236667</td>\n",
       "      <td>1012.443333</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Miami Lakes, FL</td>\n",
       "      <td>Miami Lakes</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>FL</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Helena Valley Northeast, MT</td>\n",
       "      <td>Helena Valley Northeast</td>\n",
       "      <td>MONTANA</td>\n",
       "      <td>MT</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>78.090000</td>\n",
       "      <td>75.096667</td>\n",
       "      <td>80.920000</td>\n",
       "      <td>8.593333</td>\n",
       "      <td>173.33</td>\n",
       "      <td>63.690000</td>\n",
       "      <td>1013.270000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Buckhead, GA</td>\n",
       "      <td>Buckhead</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>GA</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Upper Bear Creek, CO</td>\n",
       "      <td>Upper Bear Creek</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>CO</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Blaine-Birch Bay, WA</td>\n",
       "      <td>Blaine-Birch Bay</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>WA</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>La Marque, TX</td>\n",
       "      <td>La Marque</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>TX</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>El Cerrito, CA</td>\n",
       "      <td>El Cerrito</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>CA</td>\n",
       "      <td>Multi-Family (2-4 Unit)</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Monterey, CA</td>\n",
       "      <td>Monterey</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>CA</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>72.425806</td>\n",
       "      <td>69.480645</td>\n",
       "      <td>75.283871</td>\n",
       "      <td>16.080645</td>\n",
       "      <td>1548.12</td>\n",
       "      <td>77.222581</td>\n",
       "      <td>1013.612903</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>2015-09-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Hartford, CT</td>\n",
       "      <td>Hartford</td>\n",
       "      <td>CONNECTICUT</td>\n",
       "      <td>CT</td>\n",
       "      <td>Single Family Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Grandview, IL</td>\n",
       "      <td>Grandview</td>\n",
       "      <td>ILLINOIS</td>\n",
       "      <td>IL</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Beaver, WV</td>\n",
       "      <td>Beaver</td>\n",
       "      <td>WEST VIRGINIA</td>\n",
       "      <td>WV</td>\n",
       "      <td>All Residential</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2012-01-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Collegeville, PA</td>\n",
       "      <td>Collegeville</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>PA</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>Weigelstown, PA</td>\n",
       "      <td>Weigelstown</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>PA</td>\n",
       "      <td>Townhouse</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>30</td>\n",
       "      <td>place</td>\n",
       "      <td>False</td>\n",
       "      <td>University Center, VA</td>\n",
       "      <td>University Center</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>VA</td>\n",
       "      <td>Condo/Co-op</td>\n",
       "      <td>...</td>\n",
       "      <td>74.026667</td>\n",
       "      <td>71.029032</td>\n",
       "      <td>77.030000</td>\n",
       "      <td>11.861290</td>\n",
       "      <td>750.20</td>\n",
       "      <td>67.812903</td>\n",
       "      <td>1013.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PERIOD_BEGIN  PERIOD_END  PERIOD_DURATION REGION_TYPE  \\\n",
       "0    2015-10-01  2015-10-31               30       place   \n",
       "1    2014-04-01  2014-04-30               30       place   \n",
       "2    2017-10-01  2017-10-31               30       place   \n",
       "3    2024-07-01  2024-07-31               30       place   \n",
       "4    2022-01-01  2022-01-31               30       place   \n",
       "5    2020-07-01  2020-07-31               30       place   \n",
       "6    2020-02-01  2020-02-29               30       place   \n",
       "7    2024-03-01  2024-03-31               30       place   \n",
       "8    2013-08-01  2013-08-31               30       place   \n",
       "9    2014-01-01  2014-01-31               30       place   \n",
       "10   2022-06-01  2022-06-30               30       place   \n",
       "11   2021-08-01  2021-08-31               30       place   \n",
       "12   2016-06-01  2016-06-30               30       place   \n",
       "13   2015-04-01  2015-04-30               30       place   \n",
       "14   2024-10-01  2024-10-31               30       place   \n",
       "15   2023-12-01  2023-12-31               30       place   \n",
       "16   2017-08-01  2017-08-31               30       place   \n",
       "17   2015-09-01  2015-09-30               30       place   \n",
       "18   2024-04-01  2024-04-30               30       place   \n",
       "19   2020-07-01  2020-07-31               30       place   \n",
       "20   2024-05-01  2024-05-31               30       place   \n",
       "21   2014-09-01  2014-09-30               30       place   \n",
       "22   2024-07-01  2024-07-31               30       place   \n",
       "23   2014-10-01  2014-10-31               30       place   \n",
       "24   2022-12-01  2022-12-31               30       place   \n",
       "25   2018-06-01  2018-06-30               30       place   \n",
       "26   2022-12-01  2022-12-31               30       place   \n",
       "27   2020-05-01  2020-05-31               30       place   \n",
       "28   2015-02-01  2015-02-28               30       place   \n",
       "29   2014-01-01  2014-01-31               30       place   \n",
       "30   2017-02-01  2017-02-28               30       place   \n",
       "31   2018-07-01  2018-07-31               30       place   \n",
       "32   2014-03-01  2014-03-31               30       place   \n",
       "33   2024-05-01  2024-05-31               30       place   \n",
       "34   2020-06-01  2020-06-30               30       place   \n",
       "35   2022-11-01  2022-11-30               30       place   \n",
       "36   2018-08-01  2018-08-31               30       place   \n",
       "37   2024-06-01  2024-06-30               30       place   \n",
       "38   2017-04-01  2017-04-30               30       place   \n",
       "39   2017-10-01  2017-10-31               30       place   \n",
       "40   2020-05-01  2020-05-31               30       place   \n",
       "41   2018-12-01  2018-12-31               30       place   \n",
       "42   2021-12-01  2021-12-31               30       place   \n",
       "43   2023-07-01  2023-07-31               30       place   \n",
       "44   2015-09-01  2015-09-30               30       place   \n",
       "45   2013-04-01  2013-04-30               30       place   \n",
       "46   2021-12-01  2021-12-31               30       place   \n",
       "47   2012-01-01  2012-01-31               30       place   \n",
       "48   2018-10-01  2018-10-31               30       place   \n",
       "49   2016-08-01  2016-08-31               30       place   \n",
       "\n",
       "    IS_SEASONALLY_ADJUSTED                                             REGION  \\\n",
       "0                    False                                   Indian Trail, NC   \n",
       "1                    False                                    Hainesville, IL   \n",
       "2                    False                                      Maysville, NC   \n",
       "3                    False                                       Chewelah, WA   \n",
       "4                    False                                        Lombard, IL   \n",
       "5                    False  Augusta-Richmond County consolidated governmen...   \n",
       "6                    False                                        Concord, NC   \n",
       "7                    False                                     Eldersburg, MD   \n",
       "8                    False                                        Laplace, LA   \n",
       "9                    False                                Egypt Lake-Leto, FL   \n",
       "10                   False                                           Dupo, IL   \n",
       "11                   False                                Sea Ranch Lakes, FL   \n",
       "12                   False                                    Monroeville, OH   \n",
       "13                   False                            Chevy Chase Village, MD   \n",
       "14                   False                                    Newburyport, MA   \n",
       "15                   False                                       Meridian, PA   \n",
       "16                   False                                    Yorba Linda, CA   \n",
       "17                   False                                        Elkhart, IN   \n",
       "18                   False                                      Salisbury, NC   \n",
       "19                   False                                       Wellford, SC   \n",
       "20                   False                                         Sunray, OK   \n",
       "21                   False                                         Hudson, WI   \n",
       "22                   False                                      Rossville, MD   \n",
       "23                   False                                        Wilmore, KY   \n",
       "24                   False                                Van Bibber Lake, IN   \n",
       "25                   False                                           Troy, IL   \n",
       "26                   False                                          Osseo, MN   \n",
       "27                   False                                   Spring House, PA   \n",
       "28                   False                                           Polo, MO   \n",
       "29                   False                                      Edgartown, MA   \n",
       "30                   False                                       Damascus, OR   \n",
       "31                   False                                    Indian Lake, PA   \n",
       "32                   False                                         Saline, MI   \n",
       "33                   False                                     Manchester, CT   \n",
       "34                   False                                          Selma, CA   \n",
       "35                   False                                          Keats, KS   \n",
       "36                   False                                    Miami Lakes, FL   \n",
       "37                   False                        Helena Valley Northeast, MT   \n",
       "38                   False                                       Buckhead, GA   \n",
       "39                   False                               Upper Bear Creek, CO   \n",
       "40                   False                               Blaine-Birch Bay, WA   \n",
       "41                   False                                      La Marque, TX   \n",
       "42                   False                                     El Cerrito, CA   \n",
       "43                   False                                       Monterey, CA   \n",
       "44                   False                                       Hartford, CT   \n",
       "45                   False                                      Grandview, IL   \n",
       "46                   False                                         Beaver, WV   \n",
       "47                   False                                   Collegeville, PA   \n",
       "48                   False                                    Weigelstown, PA   \n",
       "49                   False                              University Center, VA   \n",
       "\n",
       "                                                 CITY           STATE  \\\n",
       "0                                        Indian Trail  NORTH CAROLINA   \n",
       "1                                         Hainesville        ILLINOIS   \n",
       "2                                           Maysville  NORTH CAROLINA   \n",
       "3                                            Chewelah      WASHINGTON   \n",
       "4                                             Lombard        ILLINOIS   \n",
       "5   Augusta-Richmond County Consolidated Governmen...         GEORGIA   \n",
       "6                                             Concord  NORTH CAROLINA   \n",
       "7                                          Eldersburg        MARYLAND   \n",
       "8                                             Laplace       LOUISIANA   \n",
       "9                                     Egypt Lake-Leto         FLORIDA   \n",
       "10                                               Dupo        ILLINOIS   \n",
       "11                                    Sea Ranch Lakes         FLORIDA   \n",
       "12                                        Monroeville            OHIO   \n",
       "13                                Chevy Chase Village        MARYLAND   \n",
       "14                                        Newburyport   MASSACHUSETTS   \n",
       "15                                           Meridian    PENNSYLVANIA   \n",
       "16                                        Yorba Linda      CALIFORNIA   \n",
       "17                                            Elkhart         INDIANA   \n",
       "18                                          Salisbury  NORTH CAROLINA   \n",
       "19                                           Wellford  SOUTH CAROLINA   \n",
       "20                                             Sunray        OKLAHOMA   \n",
       "21                                             Hudson       WISCONSIN   \n",
       "22                                          Rossville        MARYLAND   \n",
       "23                                            Wilmore        KENTUCKY   \n",
       "24                                    Van Bibber Lake         INDIANA   \n",
       "25                                               Troy        ILLINOIS   \n",
       "26                                              Osseo       MINNESOTA   \n",
       "27                                       Spring House    PENNSYLVANIA   \n",
       "28                                               Polo        MISSOURI   \n",
       "29                                          Edgartown   MASSACHUSETTS   \n",
       "30                                           Damascus          OREGON   \n",
       "31                                        Indian Lake    PENNSYLVANIA   \n",
       "32                                             Saline        MICHIGAN   \n",
       "33                                         Manchester     CONNECTICUT   \n",
       "34                                              Selma      CALIFORNIA   \n",
       "35                                              Keats          KANSAS   \n",
       "36                                        Miami Lakes         FLORIDA   \n",
       "37                            Helena Valley Northeast         MONTANA   \n",
       "38                                           Buckhead         GEORGIA   \n",
       "39                                   Upper Bear Creek        COLORADO   \n",
       "40                                   Blaine-Birch Bay      WASHINGTON   \n",
       "41                                          La Marque           TEXAS   \n",
       "42                                         El Cerrito      CALIFORNIA   \n",
       "43                                           Monterey      CALIFORNIA   \n",
       "44                                           Hartford     CONNECTICUT   \n",
       "45                                          Grandview        ILLINOIS   \n",
       "46                                             Beaver   WEST VIRGINIA   \n",
       "47                                       Collegeville    PENNSYLVANIA   \n",
       "48                                        Weigelstown    PENNSYLVANIA   \n",
       "49                                  University Center        VIRGINIA   \n",
       "\n",
       "   STATE_CODE              PROPERTY_TYPE  ...   avg_temp   min_temp  \\\n",
       "0          NC  Single Family Residential  ...  74.026667  71.029032   \n",
       "1          IL                Condo/Co-op  ...  74.026667  71.029032   \n",
       "2          NC            All Residential  ...  74.026667  71.029032   \n",
       "3          WA  Single Family Residential  ...  80.354839  77.393548   \n",
       "4          IL  Single Family Residential  ...  74.026667  71.029032   \n",
       "5          GA  Single Family Residential  ...  74.026667  71.029032   \n",
       "6          NC  Single Family Residential  ...  74.026667  71.029032   \n",
       "7          MD                Condo/Co-op  ...  64.609677  61.400000   \n",
       "8          LA  Single Family Residential  ...  74.026667  71.029032   \n",
       "9          FL            All Residential  ...  74.026667  71.029032   \n",
       "10         IL    Multi-Family (2-4 Unit)  ...  73.853333  70.796667   \n",
       "11         FL            All Residential  ...  74.026667  71.029032   \n",
       "12         OH            All Residential  ...  74.026667  71.029032   \n",
       "13         MD  Single Family Residential  ...  74.026667  71.029032   \n",
       "14         MA                Condo/Co-op  ...  83.667742  80.909677   \n",
       "15         PA            All Residential  ...  76.264516  73.219355   \n",
       "16         CA                  Townhouse  ...  74.026667  71.029032   \n",
       "17         IN  Single Family Residential  ...  74.026667  71.029032   \n",
       "18         NC  Single Family Residential  ...  61.553333  58.540000   \n",
       "19         SC            All Residential  ...  74.026667  71.029032   \n",
       "20         OK  Single Family Residential  ...  66.270968  63.287097   \n",
       "21         WI            All Residential  ...  74.026667  71.029032   \n",
       "22         MD                  Townhouse  ...  78.764516  75.603226   \n",
       "23         KY  Single Family Residential  ...  74.026667  71.029032   \n",
       "24         IN  Single Family Residential  ...  74.132258  71.203226   \n",
       "25         IL            All Residential  ...  74.026667  71.029032   \n",
       "26         MN                  Townhouse  ...  77.835484  74.712903   \n",
       "27         PA  Single Family Residential  ...  74.026667  71.029032   \n",
       "28         MO            All Residential  ...  74.026667  71.029032   \n",
       "29         MA            All Residential  ...  74.026667  71.029032   \n",
       "30         OR  Single Family Residential  ...  74.026667  71.029032   \n",
       "31         PA  Single Family Residential  ...  74.026667  71.029032   \n",
       "32         MI            All Residential  ...  74.026667  71.029032   \n",
       "33         CT    Multi-Family (2-4 Unit)  ...  70.880645  67.822581   \n",
       "34         CA            All Residential  ...  74.026667  71.029032   \n",
       "35         KS  Single Family Residential  ...  81.483333  78.593333   \n",
       "36         FL                  Townhouse  ...  74.026667  71.029032   \n",
       "37         MT            All Residential  ...  78.090000  75.096667   \n",
       "38         GA  Single Family Residential  ...  74.026667  71.029032   \n",
       "39         CO  Single Family Residential  ...  74.026667  71.029032   \n",
       "40         WA  Single Family Residential  ...  74.026667  71.029032   \n",
       "41         TX            All Residential  ...  74.026667  71.029032   \n",
       "42         CA    Multi-Family (2-4 Unit)  ...  74.026667  71.029032   \n",
       "43         CA            All Residential  ...  72.425806  69.480645   \n",
       "44         CT  Single Family Residential  ...  74.026667  71.029032   \n",
       "45         IL            All Residential  ...  74.026667  71.029032   \n",
       "46         WV            All Residential  ...  74.026667  71.029032   \n",
       "47         PA                  Townhouse  ...  74.026667  71.029032   \n",
       "48         PA                  Townhouse  ...  74.026667  71.029032   \n",
       "49         VA                Condo/Co-op  ...  74.026667  71.029032   \n",
       "\n",
       "     max_temp  wind_speed  precipitation   humidity     pressure  \\\n",
       "0   77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "1   77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "2   77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "3   83.332258   10.232258         539.54  64.106452  1013.070968   \n",
       "4   77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "5   77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "6   77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "7   67.477419   11.461290         716.18  66.877419  1013.167742   \n",
       "8   77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "9   77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "10  76.860000   11.223333         631.52  66.506667  1013.066667   \n",
       "11  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "12  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "13  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "14  86.545161   11.196774         657.50  65.558065  1012.887097   \n",
       "15  79.454839   11.432258         659.08  68.280645  1012.406452   \n",
       "16  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "17  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "18  64.396667   14.373333        1268.27  73.600000  1013.456667   \n",
       "19  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "20  69.241935   13.325806        1073.85  70.467742  1012.890323   \n",
       "21  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "22  81.812903   11.609677         714.24  67.829032  1012.741935   \n",
       "23  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "24  77.109677   11.987097         831.38  68.670968  1012.641935   \n",
       "25  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "26  80.777419   10.261290         413.62  64.470968  1012.896774   \n",
       "27  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "28  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "29  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "30  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "31  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "32  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "33  74.019355   10.267742         418.80  63.225806  1012.996774   \n",
       "34  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "35  84.393333   10.610000         463.92  65.236667  1012.443333   \n",
       "36  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "37  80.920000    8.593333         173.33  63.690000  1013.270000   \n",
       "38  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "39  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "40  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "41  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "42  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "43  75.283871   16.080645        1548.12  77.222581  1013.612903   \n",
       "44  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "45  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "46  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "47  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "48  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "49  77.030000   11.861290         750.20  67.812903  1013.000000   \n",
       "\n",
       "    disaster_score  source_count  weather_data_filled  \n",
       "0             13.0           NaN                 True  \n",
       "1             13.0           NaN                 True  \n",
       "2             13.0           NaN                 True  \n",
       "3              9.0          31.0                False  \n",
       "4             13.0           NaN                 True  \n",
       "5             13.0           NaN                 True  \n",
       "6             13.0           NaN                 True  \n",
       "7             12.0          31.0                False  \n",
       "8             13.0           NaN                 True  \n",
       "9             13.0           NaN                 True  \n",
       "10            11.0          30.0                False  \n",
       "11            13.0           NaN                 True  \n",
       "12            13.0           NaN                 True  \n",
       "13            13.0           NaN                 True  \n",
       "14            11.0          31.0                False  \n",
       "15            11.0          31.0                False  \n",
       "16            13.0           NaN                 True  \n",
       "17            13.0           NaN                 True  \n",
       "18            22.0          30.0                False  \n",
       "19            13.0           NaN                 True  \n",
       "20            18.0          31.0                False  \n",
       "21            13.0           NaN                 True  \n",
       "22            12.0          31.0                False  \n",
       "23            13.0           NaN                 True  \n",
       "24            14.0          31.0                False  \n",
       "25            13.0           NaN                 True  \n",
       "26             7.0          31.0                False  \n",
       "27            13.0           NaN                 True  \n",
       "28            13.0           NaN                 True  \n",
       "29            13.0           NaN                 True  \n",
       "30            13.0           NaN                 True  \n",
       "31            13.0           NaN                 True  \n",
       "32            13.0           NaN                 True  \n",
       "33             7.0          31.0                False  \n",
       "34            13.0           NaN                 True  \n",
       "35             8.0          30.0                False  \n",
       "36            13.0           NaN                 True  \n",
       "37             3.0          30.0                False  \n",
       "38            13.0           NaN                 True  \n",
       "39            13.0           NaN                 True  \n",
       "40            13.0           NaN                 True  \n",
       "41            13.0           NaN                 True  \n",
       "42            13.0           NaN                 True  \n",
       "43            26.0          31.0                False  \n",
       "44            13.0           NaN                 True  \n",
       "45            13.0           NaN                 True  \n",
       "46            13.0           NaN                 True  \n",
       "47            13.0           NaN                 True  \n",
       "48            13.0           NaN                 True  \n",
       "49            13.0           NaN                 True  \n",
       "\n",
       "[50 rows x 69 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"/Users/shivangi/Downloads/filled_redfin_noaa_data.csv\")\n",
    "df.head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c006dcc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved first 500 rows to preview_first_500_rows.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load only the first 500 rows\n",
    "df = pd.read_csv(\"/Users/shivangi/Downloads/filled_redfin_noaa_data.csv\", nrows=500)\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel(\"/Users/shivangi/Downloads/preview_first_500_rows.xlsx\", index=False)\n",
    "\n",
    "print(\"Saved first 500 rows to preview_first_500_rows.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a344fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     CITY       STATE  RISK_SCORE\n",
      "492992          La Quinta  CALIFORNIA        95.0\n",
      "1975099  Huntington Beach  CALIFORNIA        95.0\n",
      "4048912         La Quinta  CALIFORNIA        95.0\n",
      "582772          La Quinta  CALIFORNIA        95.0\n",
      "2406476       Lake Forest  CALIFORNIA        95.0\n",
      "2628450  Huntington Beach  CALIFORNIA        95.0\n",
      "2680375        Santa Rosa  CALIFORNIA        95.0\n",
      "5149211         Vacaville  CALIFORNIA        95.0\n",
      "2877050  Huntington Beach  CALIFORNIA        95.0\n",
      "3021086  Huntington Beach  CALIFORNIA        95.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Optimized risk calculation parameters\n",
    "RISK_WEIGHTS = {\n",
    "    'fema_disaster_count': {\n",
    "        'thresholds': [0, 2, 5, 10, 20],\n",
    "        'scores':    [10, 20, 35, 50, 60]\n",
    "    },\n",
    "    'natural_disaster_score': {\n",
    "        'thresholds': [0, 10, 20, 30, 40],\n",
    "        'scores':    [15, 30, 45, 60, 70]\n",
    "    },\n",
    "    'precipitation': {\n",
    "        'thresholds': [0, 2, 4, 6, 8],\n",
    "        'scores':    [10, 20, 30, 40, 50]\n",
    "    },\n",
    "    'INVENTORY': {\n",
    "        'thresholds': [0, 50, 100, 200, 300],\n",
    "        'scores':    [5, 10, 15, 20, 25]\n",
    "    }\n",
    "}\n",
    "\n",
    "def calculate_balanced_risk(row):\n",
    "    \"\"\"Calculate risk with compressed scaling and minimum baseline\"\"\"\n",
    "    total = 0\n",
    "    for feature, params in RISK_WEIGHTS.items():\n",
    "        value = row[feature]\n",
    "        for i, threshold in enumerate(params['thresholds']):\n",
    "            if value < threshold:\n",
    "                total += params['scores'][i-1] if i > 0 else 0\n",
    "                break\n",
    "        else:\n",
    "            total += params['scores'][-1]\n",
    "    \n",
    "    # Apply square root compression and normalize\n",
    "    compressed = np.sqrt(total)\n",
    "    max_possible = np.sqrt(sum(v['scores'][-1] for v in RISK_WEIGHTS.values()))\n",
    "    return min(95, (compressed / max_possible) * 100)  # Hard cap at 95\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('/Users/shivangi/Downloads/filled_redfin_noaa_data.csv')\n",
    "\n",
    "# Calculate scores\n",
    "df['RISK_SCORE'] = df.apply(calculate_balanced_risk, axis=1)\n",
    "\n",
    "# Add minimum score floor of 5\n",
    "df['RISK_SCORE'] = df['RISK_SCORE'].clip(lower=5)\n",
    "\n",
    "print(df[['CITY', 'STATE', 'RISK_SCORE']]\n",
    "      .sort_values('RISK_SCORE', ascending=False)\n",
    "      .head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2604f20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    value\n",
      "PERIOD_DURATION                     30.00\n",
      "MEDIAN_SALE_PRICE               899880.27\n",
      "MEDIAN_SALE_PRICE_MOM                0.02\n",
      "MEDIAN_SALE_PRICE_YOY                0.09\n",
      "MEDIAN_LIST_PRICE               894038.03\n",
      "MEDIAN_LIST_PRICE_MOM                0.01\n",
      "MEDIAN_LIST_PRICE_YOY                0.07\n",
      "MEDIAN_PPSF                        485.43\n",
      "MEDIAN_PPSF_MOM                      0.02\n",
      "MEDIAN_PPSF_YOY                      0.08\n",
      "MEDIAN_LIST_PPSF                   522.69\n",
      "MEDIAN_LIST_PPSF_MOM                 0.01\n",
      "MEDIAN_LIST_PPSF_YOY                 0.07\n",
      "HOMES_SOLD                          64.95\n",
      "HOMES_SOLD_MOM                       0.11\n",
      "HOMES_SOLD_YOY                       0.17\n",
      "PENDING_SALES                       75.20\n",
      "PENDING_SALES_MOM                    0.07\n",
      "PENDING_SALES_YOY                    0.10\n",
      "NEW_LISTINGS                        73.92\n",
      "NEW_LISTINGS_MOM                     0.09\n",
      "NEW_LISTINGS_YOY                     0.08\n",
      "INVENTORY                          150.12\n",
      "INVENTORY_MOM                        0.01\n",
      "INVENTORY_YOY                        0.10\n",
      "MONTHS_OF_SUPPLY                     2.93\n",
      "MONTHS_OF_SUPPLY_MOM                -0.04\n",
      "MONTHS_OF_SUPPLY_YOY                -0.19\n",
      "MEDIAN_DOM                          47.82\n",
      "MEDIAN_DOM_MOM                      -0.19\n",
      "MEDIAN_DOM_YOY                      -1.73\n",
      "AVG_SALE_TO_LIST                     0.99\n",
      "AVG_SALE_TO_LIST_MOM                 0.00\n",
      "AVG_SALE_TO_LIST_YOY                 0.00\n",
      "SOLD_ABOVE_LIST                      0.27\n",
      "SOLD_ABOVE_LIST_MOM                  0.00\n",
      "SOLD_ABOVE_LIST_YOY                  0.01\n",
      "PRICE_DROPS                          0.22\n",
      "PRICE_DROPS_MOM                      0.00\n",
      "PRICE_DROPS_YOY                     -0.01\n",
      "OFF_MARKET_IN_TWO_WEEKS              0.35\n",
      "OFF_MARKET_IN_TWO_WEEKS_MOM          0.00\n",
      "OFF_MARKET_IN_TWO_WEEKS_YOY          0.01\n",
      "PARENT_METRO_REGION_METRO_CODE   11244.00\n",
      "YEAR                              2018.18\n",
      "fema_disaster_count                 40.37\n",
      "natural_disaster_score              30.00\n",
      "MONTH                                6.43\n",
      "avg_temp                            72.42\n",
      "min_temp                            69.43\n",
      "max_temp                            75.42\n",
      "wind_speed                          12.99\n",
      "precipitation                      970.40\n",
      "humidity                            70.06\n",
      "pressure                          1013.00\n",
      "disaster_score                      16.74\n",
      "source_count                        30.41\n",
      "data_points                        782.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded Excel file\n",
    "weather_df = pd.read_csv(\"/Users/shivangi/Downloads/filled_redfin_noaa_data.csv\")\n",
    "\n",
    "# Define the WeatherAnalyzer class\n",
    "class WeatherAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df = self.df.dropna(subset=['CITY', 'STATE'])  # Ensure basic location info\n",
    "\n",
    "    def get_city_weather_report(self, city_name, state_name=None):\n",
    "        # Standardize input\n",
    "        city_name = city_name.strip().title()\n",
    "        if state_name:\n",
    "            state_name = state_name.strip().upper()\n",
    "            data = self.df[(self.df['CITY'] == city_name) & (self.df['STATE'] == state_name)]\n",
    "        else:\n",
    "            data = self.df[self.df['CITY'] == city_name]\n",
    "\n",
    "        if data.empty:\n",
    "            return pd.DataFrame({'value': [f\"No data available for {city_name}\" + (f\", {state_name}\" if state_name else \"\")]})\n",
    "\n",
    "        # Compute summary stats for that city\n",
    "        numeric_columns = data.select_dtypes(include='number').columns\n",
    "        summary = data[numeric_columns].agg('mean').round(2)\n",
    "\n",
    "        # Add sample count\n",
    "        summary['data_points'] = len(data)\n",
    "        return summary.to_frame(name='value')\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = WeatherAnalyzer(weather_df)\n",
    "\n",
    "# Example usage: report for Manchester, CT (change values as needed)\n",
    "city_report = analyzer.get_city_weather_report(\"Huntington Beach\", \"CALIFORNIA\")\n",
    "\n",
    "print(city_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c579c15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/shivangi/Downloads/filled_redfin_noaa_data.csv...\n",
      "Merged data: 5812149 rows, 69 columns\n",
      "Found 51 available states\n",
      "Sample states: ALABAMA, ALASKA, ARIZONA, ARKANSAS, CALIFORNIA, COLORADO, COLUMBIA, CONNECTICUT, DELAWARE, FLORIDA\n",
      "\n",
      "Weather Metrics Analysis for Bantam, CONNECTICUT\n",
      "==================================================\n",
      "\n",
      "Testing with Bantam, CONNECTICUT\n",
      "Found 205 records\n",
      "Years covered: 14\n",
      "\n",
      "Weather Metrics:\n",
      "- Average Temperature (°F): 74.66 (Range: 65.20-84.74, Std: 3.18)\n",
      "- Minimum Temperature (°F): 71.67 (Range: 62.19-81.83, Std: 3.19)\n",
      "- Maximum Temperature (°F): 77.66 (Range: 68.33-87.68, Std: 3.18)\n",
      "- Humidity (%): 67.60 (Range: 65.58-68.51, Std: 0.58)\n",
      "- Wind Speed (mph): 11.76 (Range: 10.95-11.86, Std: 0.23)\n",
      "- Precipitation (mm/year): 728.66 (Range: 633.78-750.20, Std: 42.17)\n",
      "- Atmospheric Pressure (hPa): 1013.02 (Range: 1012.11-1013.85, Std: 0.21)\n",
      "\n",
      "Seasonal Weather Patterns:\n",
      "- Average Temperature (°F) by Season:\n",
      "  • Fall: 76.51\n",
      "  • Spring: 72.72\n",
      "  • Summer: 75.22\n",
      "  • Winter: 73.97\n",
      "- Precipitation (mm/year) by Season:\n",
      "  • Fall: 719.29\n",
      "  • Spring: 731.95\n",
      "  • Summer: 724.00\n",
      "  • Winter: 741.69\n",
      "- Humidity (%) by Season:\n",
      "  • Fall: 67.45\n",
      "  • Spring: 67.60\n",
      "  • Summer: 67.59\n",
      "  • Winter: 67.77\n",
      "\n",
      "Disaster summary:\n",
      "- Total disasters: 2255\n",
      "- Yearly average: 161.07\n",
      "- Risk level: high\n",
      "- Natural disaster score: 11.00/100\n",
      "- Overall disaster score: 12.58/100\n",
      "\n",
      "Climate Classification: Warm, Moderate, Moderate Rainfall\n",
      "\n",
      "Weather Profile Summary:\n",
      "Warm climate with moderate temperature variations. Moderate rainfall. Moderate wind conditions. Relatively stable year-round temperatures. High risk of natural disasters. \n",
      "\n",
      "Comparing Weather Metrics Across Cities:\n",
      "==================================================\n",
      "\n",
      "Testing with Bantam, CONNECTICUT\n",
      "Found 205 records\n",
      "Years covered: 14\n",
      "\n",
      "Weather Metrics:\n",
      "- Average Temperature (°F): 74.66 (Range: 65.20-84.74, Std: 3.18)\n",
      "- Minimum Temperature (°F): 71.67 (Range: 62.19-81.83, Std: 3.19)\n",
      "- Maximum Temperature (°F): 77.66 (Range: 68.33-87.68, Std: 3.18)\n",
      "- Humidity (%): 67.60 (Range: 65.58-68.51, Std: 0.58)\n",
      "- Wind Speed (mph): 11.76 (Range: 10.95-11.86, Std: 0.23)\n",
      "- Precipitation (mm/year): 728.66 (Range: 633.78-750.20, Std: 42.17)\n",
      "- Atmospheric Pressure (hPa): 1013.02 (Range: 1012.11-1013.85, Std: 0.21)\n",
      "\n",
      "Seasonal Weather Patterns:\n",
      "- Average Temperature (°F) by Season:\n",
      "  • Fall: 76.51\n",
      "  • Spring: 72.72\n",
      "  • Summer: 75.22\n",
      "  • Winter: 73.97\n",
      "- Precipitation (mm/year) by Season:\n",
      "  • Fall: 719.29\n",
      "  • Spring: 731.95\n",
      "  • Summer: 724.00\n",
      "  • Winter: 741.69\n",
      "- Humidity (%) by Season:\n",
      "  • Fall: 67.45\n",
      "  • Spring: 67.60\n",
      "  • Summer: 67.59\n",
      "  • Winter: 67.77\n",
      "\n",
      "Disaster summary:\n",
      "- Total disasters: 2255\n",
      "- Yearly average: 161.07\n",
      "- Risk level: high\n",
      "- Natural disaster score: 11.00/100\n",
      "- Overall disaster score: 12.58/100\n",
      "\n",
      "Climate Classification: Warm, Moderate, Moderate Rainfall\n",
      "\n",
      "Weather Profile Summary:\n",
      "Warm climate with moderate temperature variations. Moderate rainfall. Moderate wind conditions. Relatively stable year-round temperatures. High risk of natural disasters. \n",
      "\n",
      "Testing with Abanda, ALABAMA\n",
      "Found 12 records\n",
      "Years covered: 6\n",
      "\n",
      "Weather Metrics:\n",
      "- Average Temperature (°F): 72.65 (Range: 65.75-74.03, Std: 3.22)\n",
      "- Minimum Temperature (°F): 69.63 (Range: 62.66-71.03, Std: 3.26)\n",
      "- Maximum Temperature (°F): 75.63 (Range: 68.60-77.03, Std: 3.28)\n",
      "- Humidity (%): 68.73 (Range: 67.81-73.34, Std: 2.15)\n",
      "- Wind Speed (mph): 12.35 (Range: 11.86-14.77, Std: 1.13)\n",
      "- Precipitation (mm/year): 844.02 (Range: 750.20-1313.14, Std: 219.12)\n",
      "- Atmospheric Pressure (hPa): 1013.05 (Range: 1013.00-1013.28, Std: 0.11)\n",
      "\n",
      "Disaster summary:\n",
      "- Total disasters: 264\n",
      "- Yearly average: 44.00\n",
      "- Risk level: high\n",
      "- Natural disaster score: 22.00/100\n",
      "- Overall disaster score: 14.50/100\n",
      "\n",
      "Climate Classification: Warm, Moderate, Wet\n",
      "\n",
      "Weather Profile Summary:\n",
      "Warm climate with moderate temperature variations. Wet and moderately humid conditions. Moderate wind conditions. High risk of natural disasters. \n",
      "\n",
      "Testing with Anchorage, ALASKA\n",
      "Found 637 records\n",
      "Years covered: 14\n",
      "\n",
      "Weather Metrics:\n",
      "- Average Temperature (°F): 75.02 (Range: 67.97-88.22, Std: 3.77)\n",
      "- Minimum Temperature (°F): 72.02 (Range: 64.81-85.31, Std: 3.77)\n",
      "- Maximum Temperature (°F): 78.03 (Range: 71.12-91.28, Std: 3.76)\n",
      "- Humidity (%): 66.62 (Range: 59.89-67.81, Std: 2.28)\n",
      "- Wind Speed (mph): 11.29 (Range: 8.92-11.86, Std: 1.08)\n",
      "- Precipitation (mm/year): 635.56 (Range: 210.46-750.20, Std: 215.21)\n",
      "- Atmospheric Pressure (hPa): 1013.00 (Range: 1012.06-1013.57, Std: 0.17)\n",
      "\n",
      "Seasonal Weather Patterns:\n",
      "- Average Temperature (°F) by Season:\n",
      "  • Fall: 76.94\n",
      "  • Spring: 73.25\n",
      "  • Summer: 75.89\n",
      "  • Winter: 74.09\n",
      "- Precipitation (mm/year) by Season:\n",
      "  • Fall: 627.83\n",
      "  • Spring: 646.87\n",
      "  • Summer: 631.48\n",
      "  • Winter: 635.80\n",
      "- Humidity (%) by Season:\n",
      "  • Fall: 66.47\n",
      "  • Spring: 66.72\n",
      "  • Summer: 66.62\n",
      "  • Winter: 66.66\n",
      "\n",
      "Disaster summary:\n",
      "- Total disasters: 2548\n",
      "- Yearly average: 182.00\n",
      "- Risk level: high\n",
      "- Natural disaster score: 4.00/100\n",
      "- Overall disaster score: 11.01/100\n",
      "\n",
      "Climate Classification: Hot, Moderate, Moderate Rainfall\n",
      "\n",
      "Weather Profile Summary:\n",
      "Hot climate with moderate temperature variations. Moderate rainfall. Moderate wind conditions. Relatively stable year-round temperatures. High risk of natural disasters. \n",
      "\n",
      "Testing with Aguila, ARIZONA\n",
      "Found 14 records\n",
      "Years covered: 7\n",
      "\n",
      "Weather Metrics:\n",
      "- Average Temperature (°F): 74.67 (Range: 70.35-82.19, Std: 3.45)\n",
      "- Minimum Temperature (°F): 71.66 (Range: 67.42-79.09, Std: 3.40)\n",
      "- Maximum Temperature (°F): 77.68 (Range: 73.44-85.15, Std: 3.42)\n",
      "- Humidity (%): 67.85 (Range: 67.18-68.70, Std: 0.42)\n",
      "- Wind Speed (mph): 11.75 (Range: 11.39-11.86, Std: 0.19)\n",
      "- Precipitation (mm/year): 739.32 (Range: 709.71-750.20, Std: 17.90)\n",
      "- Atmospheric Pressure (hPa): 1012.93 (Range: 1012.70-1013.00, Std: 0.12)\n",
      "\n",
      "Disaster summary:\n",
      "- Total disasters: 168\n",
      "- Yearly average: 24.00\n",
      "- Risk level: high\n",
      "- Natural disaster score: 12.00/100\n",
      "- Overall disaster score: 12.71/100\n",
      "\n",
      "Climate Classification: Warm, Moderate, Moderate Rainfall\n",
      "\n",
      "Weather Profile Summary:\n",
      "Warm climate with moderate temperature variations. Moderate rainfall. Moderate wind conditions. High risk of natural disasters. \n",
      "\n",
      "Key Weather Metrics Comparison:\n",
      "--------------------------------------------------\n",
      "\n",
      "Avg Temp:\n",
      "  Anchorage, ALASKA: 75.02\n",
      "  Aguila, ARIZONA: 74.67\n",
      "  Bantam, CONNECTICUT: 74.66\n",
      "  Abanda, ALABAMA: 72.65\n",
      "\n",
      "Precipitation:\n",
      "  Abanda, ALABAMA: 844.02\n",
      "  Aguila, ARIZONA: 739.32\n",
      "  Bantam, CONNECTICUT: 728.66\n",
      "  Anchorage, ALASKA: 635.56\n",
      "\n",
      "Humidity:\n",
      "  Abanda, ALABAMA: 68.73\n",
      "  Aguila, ARIZONA: 67.85\n",
      "  Bantam, CONNECTICUT: 67.60\n",
      "  Anchorage, ALASKA: 66.62\n",
      "\n",
      "Wind Speed:\n",
      "  Abanda, ALABAMA: 12.35\n",
      "  Bantam, CONNECTICUT: 11.76\n",
      "  Aguila, ARIZONA: 11.75\n",
      "  Anchorage, ALASKA: 11.29\n",
      "\n",
      "Disaster Risk Comparison:\n",
      "--------------------------------------------------\n",
      "  Anchorage, ALASKA: 182.00 disasters/year (HIGH risk)\n",
      "  Bantam, CONNECTICUT: 161.07 disasters/year (HIGH risk)\n",
      "  Abanda, ALABAMA: 44.00 disasters/year (HIGH risk)\n",
      "  Aguila, ARIZONA: 24.00 disasters/year (HIGH risk)\n",
      "\n",
      "Climate Similarity:\n",
      "--------------------------------------------------\n",
      "  Bantam, CONNECTICUT & Abanda, ALABAMA: Very Similar\n",
      "  Bantam, CONNECTICUT & Anchorage, ALASKA: Very Similar\n",
      "  Bantam, CONNECTICUT & Aguila, ARIZONA: Very Similar\n",
      "  Abanda, ALABAMA & Bantam, CONNECTICUT: Very Similar\n",
      "  Abanda, ALABAMA & Anchorage, ALASKA: Very Similar\n",
      "  Abanda, ALABAMA & Aguila, ARIZONA: Very Similar\n",
      "  Anchorage, ALASKA & Bantam, CONNECTICUT: Very Similar\n",
      "  Anchorage, ALASKA & Abanda, ALABAMA: Very Similar\n",
      "  Anchorage, ALASKA & Aguila, ARIZONA: Very Similar\n",
      "  Aguila, ARIZONA & Bantam, CONNECTICUT: Very Similar\n",
      "  Aguila, ARIZONA & Abanda, ALABAMA: Very Similar\n",
      "  Aguila, ARIZONA & Anchorage, ALASKA: Very Similar\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "class WeatherMetricsAnalyzer:\n",
    "    def __init__(self, data_path=\"/Users/shivangi/Downloads/filled_redfin_noaa_data.csv\"):\n",
    "        \"\"\"\n",
    "        Initialize the Weather Metrics Analyzer with a merged dataset path.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : str\n",
    "            Path to the merged FEMA-Redfin-Weather dataset\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.data = None\n",
    "        self.states = []\n",
    "        self.cities_by_state = {}\n",
    "        \n",
    "        # If data path provided, load the data\n",
    "        if data_path:\n",
    "            self.load_data(data_path)\n",
    "        else:\n",
    "            print(\"No data path provided. Using sample data for demonstration.\")\n",
    "            self.generate_sample_data()\n",
    "    \n",
    "    def load_data(self, data_path):\n",
    "        \"\"\"Load the merged dataset from a CSV file.\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading data from {data_path}...\")\n",
    "            self.data = pd.read_csv(data_path)\n",
    "            print(f\"Merged data: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "            \n",
    "            # Initialize states and cities\n",
    "            self.extract_states_and_cities()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            self.generate_sample_data()\n",
    "    \n",
    "    def generate_sample_data(self, rows=5000):\n",
    "        \"\"\"Generate sample data for demonstration purposes.\"\"\"\n",
    "        print(f\"Generating sample data with {rows} rows...\")\n",
    "        \n",
    "        # Define sample states and cities\n",
    "        sample_states = [\"ALABAMA\", \"ALASKA\", \"ARIZONA\", \"ARKANSAS\", \"CALIFORNIA\", \n",
    "                         \"COLORADO\", \"CONNECTICUT\", \"DELAWARE\", \"FLORIDA\", \"GEORGIA\"]\n",
    "        \n",
    "        cities_by_state = {\n",
    "            \"CONNECTICUT\": [\"Ansonia\", \"Ball Pond\", \"Baltic\", \"Bantam\", \"Bethel\", \n",
    "                           \"Middletown\", \"Hartford\", \"Bristol\", \"Norwich\", \"Manchester\"],\n",
    "            \"CALIFORNIA\": [\"Los Angeles\", \"San Francisco\", \"San Diego\", \"Sacramento\", \n",
    "                          \"Oakland\", \"San Jose\", \"Fresno\", \"Long Beach\", \"Bakersfield\"],\n",
    "            \"FLORIDA\": [\"Miami\", \"Orlando\", \"Tampa\", \"Jacksonville\", \"Fort Lauderdale\", \n",
    "                       \"Tallahassee\", \"Naples\", \"Key West\", \"Pensacola\", \"Gainesville\"]\n",
    "        }\n",
    "        \n",
    "        # Add random cities for other states\n",
    "        for state in sample_states:\n",
    "            if state not in cities_by_state:\n",
    "                cities_by_state[state] = [f\"City_{i}\" for i in range(1, 11)]\n",
    "        \n",
    "        # Create empty dataframe\n",
    "        self.data = pd.DataFrame()\n",
    "        \n",
    "        # Generate random data\n",
    "        periods = pd.date_range(start='2010-01-01', end='2023-12-31', freq='M')\n",
    "        \n",
    "        data_rows = []\n",
    "        for _ in range(rows):\n",
    "            state = random.choice(sample_states)\n",
    "            city = random.choice(cities_by_state[state])\n",
    "            period = random.choice(periods)\n",
    "            \n",
    "            # Generate realistic weather data based on region\n",
    "            if state == \"FLORIDA\":\n",
    "                avg_temp = np.random.normal(72, 5)  # Close to your avg_temp value\n",
    "                min_temp = avg_temp - np.random.normal(3, 1)  # Similar to your spread\n",
    "                max_temp = avg_temp + np.random.normal(3, 1)\n",
    "                humidity = np.random.normal(70, 5)  # Close to your humidity value\n",
    "                wind_speed = np.random.normal(13, 3)  # Similar to your wind_speed\n",
    "                precipitation = np.random.normal(970, 200)  # Matching your precipitation scale\n",
    "                pressure = np.random.normal(1013, 2)  # Matches your pressure value\n",
    "            elif state == \"CALIFORNIA\":\n",
    "                avg_temp = np.random.normal(65, 7)\n",
    "                min_temp = avg_temp - np.random.normal(3, 1)\n",
    "                max_temp = avg_temp + np.random.normal(3, 1)\n",
    "                humidity = np.random.normal(60, 8)\n",
    "                wind_speed = np.random.normal(10, 4)\n",
    "                precipitation = np.random.normal(500, 300)\n",
    "                pressure = np.random.normal(1012, 3)\n",
    "            elif state == \"CONNECTICUT\":\n",
    "                avg_temp = np.random.normal(55, 15)\n",
    "                min_temp = avg_temp - np.random.normal(4, 2)\n",
    "                max_temp = avg_temp + np.random.normal(4, 2)\n",
    "                humidity = np.random.normal(65, 10)\n",
    "                wind_speed = np.random.normal(14, 5)\n",
    "                precipitation = np.random.normal(1200, 300)\n",
    "                pressure = np.random.normal(1010, 5)\n",
    "            else:\n",
    "                avg_temp = np.random.normal(65, 10)\n",
    "                min_temp = avg_temp - np.random.normal(3, 2)\n",
    "                max_temp = avg_temp + np.random.normal(3, 2)\n",
    "                humidity = np.random.normal(65, 10)\n",
    "                wind_speed = np.random.normal(12, 4)\n",
    "                precipitation = np.random.normal(800, 400)\n",
    "                pressure = np.random.normal(1011, 4)\n",
    "            \n",
    "            # Generate FEMA disaster data\n",
    "            natural_disaster_score = np.random.normal(30, 10)  # Matches your natural_disaster_score\n",
    "            disaster_score = np.random.normal(17, 5)  # Close to your disaster_score\n",
    "            \n",
    "            data_rows.append({\n",
    "                \"PERIOD_BEGIN\": period,\n",
    "                \"STATE\": state,\n",
    "                \"CITY\": city,\n",
    "                \"avg_temp\": max(0, min(110, avg_temp)),\n",
    "                \"min_temp\": max(0, min(110, min_temp)),\n",
    "                \"max_temp\": max(0, min(110, max_temp)),\n",
    "                \"humidity\": max(0, min(100, humidity)),\n",
    "                \"wind_speed\": max(0, wind_speed),\n",
    "                \"precipitation\": max(0, precipitation),\n",
    "                \"pressure\": max(980, min(1040, pressure)),\n",
    "                \"natural_disaster_score\": max(0, natural_disaster_score),\n",
    "                \"disaster_score\": max(0, disaster_score),\n",
    "                \"YEAR\": period.year,\n",
    "                \"MONTH\": period.month\n",
    "            })\n",
    "        \n",
    "        self.data = pd.DataFrame(data_rows)\n",
    "        \n",
    "        # Initialize states and cities\n",
    "        self.extract_states_and_cities()\n",
    "    \n",
    "    def extract_states_and_cities(self):\n",
    "        \"\"\"Extract unique states and cities from the dataset.\"\"\"\n",
    "        if self.data is not None:\n",
    "            self.states = sorted(self.data['STATE'].unique())\n",
    "            print(f\"Found {len(self.states)} available states\")\n",
    "            print(f\"Sample states: {', '.join(self.states[:10])}\")\n",
    "            \n",
    "            # Extract cities by state\n",
    "            for state in self.states:\n",
    "                state_cities = sorted(self.data[self.data['STATE'] == state]['CITY'].unique())\n",
    "                self.cities_by_state[state] = state_cities\n",
    "    \n",
    "    def get_states(self):\n",
    "        \"\"\"Return list of available states.\"\"\"\n",
    "        return self.states\n",
    "    \n",
    "    def get_cities(self, state):\n",
    "        \"\"\"Return list of cities for a given state.\"\"\"\n",
    "        return self.cities_by_state.get(state, [])\n",
    "    \n",
    "    def analyze_state(self, state):\n",
    "        \"\"\"Analyze state-level metrics and provide summary.\"\"\"\n",
    "        if state not in self.states:\n",
    "            print(f\"State {state} not found in the dataset.\")\n",
    "            return None\n",
    "        \n",
    "        state_data = self.data[self.data['STATE'] == state]\n",
    "        cities = self.get_cities(state)\n",
    "        \n",
    "        print(f\"\\nState {state} has {len(cities)} cities\")\n",
    "        \n",
    "        # Count cities with few records\n",
    "        city_counts = state_data['CITY'].value_counts()\n",
    "        cities_with_one_record = sum(city_counts == 1)\n",
    "        cities_with_few_records = sum(city_counts < 5)\n",
    "        \n",
    "        print(f\"Cities with only 1 record: {cities_with_one_record}\")\n",
    "        print(f\"Cities with fewer than 5 records: {cities_with_few_records}\")\n",
    "        \n",
    "        # Top cities by record count\n",
    "        print(\"\\nTop cities by record count:\")\n",
    "        for city, count in city_counts.head(5).items():\n",
    "            print(f\"- {city}: {count} records\")\n",
    "        \n",
    "        print(f\"\\nSample cities: {', '.join(cities[:5])}\")\n",
    "        \n",
    "        # Disaster summary\n",
    "        years = state_data['YEAR'].nunique()\n",
    "        total_disasters = state_data['natural_disaster_score'].sum()\n",
    "        yearly_avg = total_disasters / years if years > 0 else 0\n",
    "        \n",
    "        print(f\"\\nDisaster summary for {state}:\")\n",
    "        print(f\"Total disasters: {int(total_disasters)}\")\n",
    "        print(f\"Yearly average: {yearly_avg:.2f}\")\n",
    "        print(f\"Years covered: {years}\")\n",
    "        \n",
    "        # Determine risk level\n",
    "        if yearly_avg < 10:\n",
    "            risk = \"low\"\n",
    "        elif yearly_avg < 100:\n",
    "            risk = \"medium\" \n",
    "        else:\n",
    "            risk = \"high\"\n",
    "        \n",
    "        print(f\"Risk level: {risk}\")\n",
    "        \n",
    "        return {\n",
    "            \"state\": state,\n",
    "            \"cities\": len(cities),\n",
    "            \"total_disasters\": int(total_disasters),\n",
    "            \"yearly_avg_disasters\": yearly_avg,\n",
    "            \"years_covered\": years,\n",
    "            \"risk_level\": risk\n",
    "        }\n",
    "    \n",
    "    def analyze_city_weather(self, state, city):\n",
    "        \"\"\"Analyze weather metrics for a specific city.\"\"\"\n",
    "        if state not in self.states or city not in self.get_cities(state):\n",
    "            print(f\"City {city}, {state} not found in the dataset.\")\n",
    "            return None\n",
    "        \n",
    "        city_data = self.data[(self.data['STATE'] == state) & (self.data['CITY'] == city)]\n",
    "        record_count = len(city_data)\n",
    "        \n",
    "        print(f\"\\nTesting with {city}, {state}\")\n",
    "        print(f\"Found {record_count} records\")\n",
    "        \n",
    "        # Years covered\n",
    "        years = city_data['YEAR'].nunique()\n",
    "        print(f\"Years covered: {years}\")\n",
    "        \n",
    "        # Weather metrics analysis\n",
    "        weather_metrics = {\n",
    "            'avg_temp': 'Average Temperature (°F)',\n",
    "            'min_temp': 'Minimum Temperature (°F)',\n",
    "            'max_temp': 'Maximum Temperature (°F)',\n",
    "            'humidity': 'Humidity (%)',\n",
    "            'wind_speed': 'Wind Speed (mph)',\n",
    "            'precipitation': 'Precipitation (mm/year)',\n",
    "            'pressure': 'Atmospheric Pressure (hPa)'\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(\"\\nWeather Metrics:\")\n",
    "        for metric, label in weather_metrics.items():\n",
    "            if metric in city_data.columns:\n",
    "                avg = city_data[metric].mean()\n",
    "                min_val = city_data[metric].min()\n",
    "                max_val = city_data[metric].max()\n",
    "                std = city_data[metric].std()\n",
    "                \n",
    "                results[metric] = {\n",
    "                    'avg': avg,\n",
    "                    'min': min_val,\n",
    "                    'max': max_val,\n",
    "                    'std': std\n",
    "                }\n",
    "                \n",
    "                print(f\"- {label}: {avg:.2f} (Range: {min_val:.2f}-{max_val:.2f}, Std: {std:.2f})\")\n",
    "        \n",
    "        # Seasonal analysis if we have month data\n",
    "        if 'MONTH' in city_data.columns and len(city_data) > 20:\n",
    "            print(\"\\nSeasonal Weather Patterns:\")\n",
    "            \n",
    "            # Define seasons\n",
    "            city_data['SEASON'] = city_data['MONTH'].apply(lambda m: \n",
    "                                                   'Winter' if m in [12, 1, 2] else\n",
    "                                                   'Spring' if m in [3, 4, 5] else\n",
    "                                                   'Summer' if m in [6, 7, 8] else\n",
    "                                                   'Fall')\n",
    "            \n",
    "            seasonal_metrics = {}\n",
    "            \n",
    "            for metric in ['avg_temp', 'precipitation', 'humidity']:\n",
    "                if metric in city_data.columns:\n",
    "                    seasonal_data = city_data.groupby('SEASON')[metric].mean().to_dict()\n",
    "                    seasonal_metrics[metric] = seasonal_data\n",
    "                    \n",
    "                    print(f\"- {weather_metrics[metric]} by Season:\")\n",
    "                    for season, value in seasonal_data.items():\n",
    "                        print(f\"  • {season}: {value:.2f}\")\n",
    "            \n",
    "            results['seasonal'] = seasonal_metrics\n",
    "        \n",
    "        # Disaster analysis\n",
    "        if 'natural_disaster_score' in city_data.columns:\n",
    "            total_disasters = city_data['natural_disaster_score'].sum()\n",
    "            yearly_avg = total_disasters / years if years > 0 else 0\n",
    "            \n",
    "            # Calculate risk level\n",
    "            if yearly_avg < 5:\n",
    "                risk = \"low\"\n",
    "            elif yearly_avg < 20:\n",
    "                risk = \"medium\" \n",
    "            else:\n",
    "                risk = \"high\"\n",
    "            \n",
    "            print(\"\\nDisaster summary:\")\n",
    "            print(f\"- Total disasters: {int(total_disasters)}\")\n",
    "            print(f\"- Yearly average: {yearly_avg:.2f}\")\n",
    "            print(f\"- Risk level: {risk}\")\n",
    "            \n",
    "            if 'natural_disaster_score' in city_data.columns:\n",
    "                avg_score = city_data['natural_disaster_score'].mean()\n",
    "                print(f\"- Natural disaster score: {avg_score:.2f}/100\")\n",
    "            \n",
    "            if 'disaster_score' in city_data.columns:\n",
    "                avg_score = city_data['disaster_score'].mean()\n",
    "                print(f\"- Overall disaster score: {avg_score:.2f}/100\")\n",
    "            \n",
    "            results['disasters'] = {\n",
    "                'total': int(total_disasters),\n",
    "                'yearly_avg': yearly_avg,\n",
    "                'risk_level': risk\n",
    "            }\n",
    "        \n",
    "        # Weather classification\n",
    "        if 'avg_temp' in results and 'precipitation' in results and 'humidity' in results:\n",
    "            # Temperature classification\n",
    "            avg_temp = results['avg_temp']['avg']\n",
    "            if avg_temp > 75:\n",
    "                temp_class = \"Hot\"\n",
    "            elif avg_temp > 65:\n",
    "                temp_class = \"Warm\"\n",
    "            elif avg_temp > 50:\n",
    "                temp_class = \"Mild\"\n",
    "            else:\n",
    "                temp_class = \"Cool\"\n",
    "            \n",
    "            # Humidity classification\n",
    "            avg_humid = results['humidity']['avg']\n",
    "            if avg_humid > 70:\n",
    "                humid_class = \"Humid\"\n",
    "            elif avg_humid > 50:\n",
    "                humid_class = \"Moderate\"\n",
    "            else:\n",
    "                humid_class = \"Dry\"\n",
    "            \n",
    "            # Precipitation classification\n",
    "            avg_precip = results['precipitation']['avg']\n",
    "            if avg_precip > 1200:\n",
    "                precip_class = \"Very Wet\"\n",
    "            elif avg_precip > 800:\n",
    "                precip_class = \"Wet\"\n",
    "            elif avg_precip > 500:\n",
    "                precip_class = \"Moderate Rainfall\"\n",
    "            else:\n",
    "                precip_class = \"Dry\"\n",
    "            \n",
    "            climate = f\"{temp_class}, {humid_class}, {precip_class}\"\n",
    "            print(f\"\\nClimate Classification: {climate}\")\n",
    "            \n",
    "            results['climate_class'] = climate\n",
    "        \n",
    "        # Generate weather profile summary\n",
    "        profile = self.generate_weather_profile(results)\n",
    "        print(f\"\\nWeather Profile Summary:\")\n",
    "        print(profile)\n",
    "        \n",
    "        results['profile'] = profile\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_weather_profile(self, results):\n",
    "        \"\"\"Generate a descriptive summary of the weather profile.\"\"\"\n",
    "        profile = \"\"\n",
    "        \n",
    "        # Temperature profile\n",
    "        if 'avg_temp' in results:\n",
    "            avg = results['avg_temp']['avg']\n",
    "            range_val = results['max_temp']['avg'] - results['min_temp']['avg']\n",
    "            \n",
    "            if avg > 75:\n",
    "                profile += \"Hot climate with \"\n",
    "            elif avg > 65:\n",
    "                profile += \"Warm climate with \"\n",
    "            elif avg > 50:\n",
    "                profile += \"Mild climate with \"\n",
    "            else:\n",
    "                profile += \"Cool climate with \"\n",
    "            \n",
    "            if range_val > 25:\n",
    "                profile += \"extreme temperature variations. \"\n",
    "            elif range_val > 15:\n",
    "                profile += \"significant temperature variations. \"\n",
    "            else:\n",
    "                profile += \"moderate temperature variations. \"\n",
    "        \n",
    "        # Precipitation and humidity\n",
    "        if 'precipitation' in results and 'humidity' in results:\n",
    "            precip = results['precipitation']['avg']\n",
    "            humid = results['humidity']['avg']\n",
    "            \n",
    "            if precip > 1200 and humid > 70:\n",
    "                profile += \"Very wet and humid conditions. \"\n",
    "            elif precip > 800 and humid > 65:\n",
    "                profile += \"Wet and moderately humid conditions. \"\n",
    "            elif precip > 500:\n",
    "                profile += \"Moderate rainfall. \"\n",
    "            else:\n",
    "                profile += \"Relatively dry conditions. \"\n",
    "        \n",
    "        # Wind conditions\n",
    "        if 'wind_speed' in results:\n",
    "            wind = results['wind_speed']['avg']\n",
    "            \n",
    "            if wind > 15:\n",
    "                profile += \"High average wind speeds. \"\n",
    "            elif wind > 10:\n",
    "                profile += \"Moderate wind conditions. \"\n",
    "            else:\n",
    "                profile += \"Generally calm winds. \"\n",
    "        \n",
    "        # Seasonal variations\n",
    "        if 'seasonal' in results and 'avg_temp' in results['seasonal']:\n",
    "            seasons = results['seasonal']['avg_temp']\n",
    "            max_season = max(seasons.items(), key=lambda x: x[1])[0]\n",
    "            min_season = min(seasons.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            if max_season == 'Summer' and seasons['Summer'] - seasons['Winter'] > 20:\n",
    "                profile += f\"Distinct seasons with hot summers and cold winters. \"\n",
    "            elif max_season == 'Summer' and seasons['Summer'] - seasons['Winter'] > 10:\n",
    "                profile += f\"Moderate seasonal variations. \"\n",
    "            else:\n",
    "                profile += f\"Relatively stable year-round temperatures. \"\n",
    "        \n",
    "        # Disaster risk\n",
    "        if 'disasters' in results:\n",
    "            risk = results['disasters']['risk_level']\n",
    "            \n",
    "            if risk == 'high':\n",
    "                profile += \"High risk of natural disasters. \"\n",
    "            elif risk == 'medium':\n",
    "                profile += \"Moderate risk of natural disasters. \"\n",
    "            else:\n",
    "                profile += \"Low risk of natural disasters. \"\n",
    "        \n",
    "        return profile\n",
    "    \n",
    "    def compare_cities(self, city_list):\n",
    "        \"\"\"Compare weather metrics across multiple cities.\"\"\"\n",
    "        print(\"\\nComparing Weather Metrics Across Cities:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if len(city_list) < 2:\n",
    "            print(\"Need at least 2 cities to compare.\")\n",
    "            return None\n",
    "        \n",
    "        # Collect data for each city\n",
    "        city_data = {}\n",
    "        for state, city in city_list:\n",
    "            results = self.analyze_city_weather(state, city)\n",
    "            if results:\n",
    "                city_data[f\"{city}, {state}\"] = results\n",
    "        \n",
    "        if len(city_data) < 2:\n",
    "            print(\"Not enough valid cities found for comparison.\")\n",
    "            return None\n",
    "        \n",
    "        # Compare key metrics\n",
    "        metrics_to_compare = ['avg_temp', 'precipitation', 'humidity', 'wind_speed']\n",
    "        \n",
    "        print(\"\\nKey Weather Metrics Comparison:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for metric in metrics_to_compare:\n",
    "            metric_values = {}\n",
    "            for city_name, data in city_data.items():\n",
    "                if metric in data:\n",
    "                    metric_values[city_name] = data[metric]['avg']\n",
    "            \n",
    "            if metric_values:\n",
    "                print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "                for city, value in sorted(metric_values.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(f\"  {city}: {value:.2f}\")\n",
    "        \n",
    "        # Compare disaster risk\n",
    "        print(\"\\nDisaster Risk Comparison:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        risk_data = {}\n",
    "        for city_name, data in city_data.items():\n",
    "            if 'disasters' in data:\n",
    "                risk_data[city_name] = {\n",
    "                    'yearly_avg': data['disasters']['yearly_avg'],\n",
    "                    'risk_level': data['disasters']['risk_level']\n",
    "                }\n",
    "        \n",
    "        if risk_data:\n",
    "            for city, risk in sorted(risk_data.items(), key=lambda x: x[1]['yearly_avg'], reverse=True):\n",
    "                print(f\"  {city}: {risk['yearly_avg']:.2f} disasters/year ({risk['risk_level'].upper()} risk)\")\n",
    "        \n",
    "        # Optional: Generate climate similarity matrix\n",
    "        if len(city_data) >= 3:\n",
    "            print(\"\\nClimate Similarity:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for city1 in city_data:\n",
    "                for city2 in city_data:\n",
    "                    if city1 != city2:\n",
    "                        similarity = self.calculate_climate_similarity(city_data[city1], city_data[city2])\n",
    "                        similarity_text = \"Very Similar\" if similarity > 0.8 else \\\n",
    "                                         \"Similar\" if similarity > 0.6 else \\\n",
    "                                         \"Somewhat Similar\" if similarity > 0.4 else \\\n",
    "                                         \"Different\"\n",
    "                        print(f\"  {city1} & {city2}: {similarity_text}\")\n",
    "        \n",
    "        return city_data\n",
    "    \n",
    "    def calculate_climate_similarity(self, city1_data, city2_data):\n",
    "        \"\"\"Calculate a simple climate similarity score between two cities.\"\"\"\n",
    "        metrics = ['avg_temp', 'precipitation', 'humidity', 'wind_speed']\n",
    "        similarities = []\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in city1_data and metric in city2_data:\n",
    "                val1 = city1_data[metric]['avg']\n",
    "                val2 = city2_data[metric]['avg']\n",
    "                \n",
    "                # Normalize the difference based on typical ranges\n",
    "                if metric == 'avg_temp':\n",
    "                    max_diff = 50  # Typical max temperature difference\n",
    "                elif metric == 'precipitation':\n",
    "                    max_diff = 1500  # Typical max precipitation difference\n",
    "                elif metric == 'humidity':\n",
    "                    max_diff = 70  # Typical max humidity difference\n",
    "                elif metric == 'wind_speed':\n",
    "                    max_diff = 20  # Typical max wind speed difference\n",
    "                else:\n",
    "                    max_diff = abs(val1 - val2) * 2  # Fallback\n",
    "                \n",
    "                # Calculate similarity (1 = identical, 0 = maximally different)\n",
    "                diff = abs(val1 - val2)\n",
    "                similarity = max(0, 1 - (diff / max_diff))\n",
    "                similarities.append(similarity)\n",
    "        \n",
    "        # Return average similarity if we have data\n",
    "        if similarities:\n",
    "            return sum(similarities) / len(similarities)\n",
    "        return 0\n",
    "    \n",
    "    def run_demo(self, state=None, city=None):\n",
    "        \"\"\"Run a demonstration of the weather analyzer.\"\"\"\n",
    "        if not state:\n",
    "            state = \"CONNECTICUT\" if \"CONNECTICUT\" in self.states else self.states[0]\n",
    "        \n",
    "        if not city:\n",
    "            cities = self.get_cities(state)\n",
    "            city = \"Bantam\" if \"Bantam\" in cities else cities[0] if cities else None\n",
    "        \n",
    "        if not city:\n",
    "            print(f\"No cities found for {state}. Using state-level analysis only.\")\n",
    "            self.analyze_state(state)\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nWeather Metrics Analysis for {city}, {state}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Run the analysis\n",
    "        results = self.analyze_city_weather(state, city)\n",
    "        \n",
    "        # Compare with other cities\n",
    "        other_cities = []\n",
    "        for s in self.states[:3]:  # Use first 3 states\n",
    "            if s != state:\n",
    "                cities = self.get_cities(s)\n",
    "                if cities:\n",
    "                    other_cities.append((s, cities[0]))\n",
    "        \n",
    "        if other_cities:\n",
    "            compare_list = [(state, city)] + other_cities\n",
    "            self.compare_cities(compare_list)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create analyzer with sample data\n",
    "    analyzer = WeatherMetricsAnalyzer()\n",
    "    \n",
    "    # Run demo with Connecticut and Bantam\n",
    "    analyzer.run_demo(\"CONNECTICUT\", \"Bantam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598dcb34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86229681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ff189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
